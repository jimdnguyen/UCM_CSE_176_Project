{"cells":[{"cell_type":"markdown","metadata":{"id":"S0ksioPziChR"},"source":["Took the first 3 blocks of code from Yerlan's LC Compression Collab\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlMXQhMKh2-B","executionInfo":{"status":"ok","timestamp":1639791938553,"user_tz":480,"elapsed":1460,"user":{"displayName":"Hex Man","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01194659618639641575"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3925802f-0ea1-4170-ee03-57396efd2f37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'LC-model-compression'...\n","remote: Enumerating objects: 166, done.\u001b[K\n","remote: Counting objects: 100% (166/166), done.\u001b[K\n","remote: Compressing objects: 100% (121/121), done.\u001b[K\n","remote: Total 166 (delta 63), reused 142 (delta 42), pack-reused 0\u001b[K\n","Receiving objects: 100% (166/166), 3.19 MiB | 9.83 MiB/s, done.\n","Resolving deltas: 100% (63/63), done.\n"]}],"source":["#! git clone https://github.com/UCMerced-ML/LC-model-compression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZELCKkPh5eT","executionInfo":{"status":"ok","timestamp":1639791955343,"user_tz":480,"elapsed":15198,"user":{"displayName":"Hex Man","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01194659618639641575"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b4fd3e1-054e-41c8-97d4-e404ba046cc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/LC-model-compression\n","Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from lc==0.1) (1.19.5)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from lc==0.1) (1.4.1)\n","Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from lc==0.1) (1.10.0+cu111)\n","Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from lc==0.1) (0.11.1+cu111)\n","Requirement already satisfied: scikit-learn>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from lc==0.1) (1.0.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->lc==0.1) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.2->lc==0.1) (1.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->lc==0.1) (3.10.0.2)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.2->lc==0.1) (7.1.2)\n","Installing collected packages: lc\n","  Running setup.py develop for lc\n","Successfully installed lc-0.1\n"]}],"source":["# ! pip3 install -e ./LC-model-compression"]},{"cell_type":"markdown","metadata":{"id":"xbioGuw1h8oU"},"source":["## IMPORTANT!\n","At this point you need to restart the runtime by doing \"Runtime => Restart Runtime\"\n","\n","After doing the restart, I recommend just commenting out the 2 lines of code above so that we can press run all"]},{"cell_type":"code","source":[""],"metadata":{"id":"SUbnxY7IUTwl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x3iOsCuGrNmv"},"source":["## Training the Neural Network"]},{"cell_type":"markdown","metadata":{"id":"CO9hxJGVrSE0"},"source":["### Import packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qKzjYwwIfHF"},"outputs":[],"source":["#%matplotlib inline\n","\n","import lc\n","from lc.torch import ParameterTorch as Param, AsVector, AsIs\n","from lc.compression_types import ConstraintL0Pruning, LowRank, RankSelection, AdaptiveQuantization\n","\n","import torch\n","from torch import optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","from timeit import default_timer as timer\n","from datetime import timedelta\n","from prettytable import PrettyTable\n","\n","torch.manual_seed(0)\n","np.random.seed(0)\n","torch.set_num_threads(4)\n","batchsize = 2048\n","num_workers = 2\n","nnloss = torch.nn.CrossEntropyLoss()\n"]},{"cell_type":"code","source":["#https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model\n","def count_parameters(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        param = parameter.numel()\n","        table.add_row([name, param])\n","        total_params+=param\n","    print(table)\n","    print(f\"Total Trainable Params: {total_params}\")\n","    return total_params\n","#https://stackabuse.com/python-check-if-string-contains-substring/\n","def count_parameters_no_bias(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        if name != None and 'bias' in name: continue \n","        else:\n","          param = parameter.numel()\n","          table.add_row([name, param])\n","          total_params+=param\n","    print(table)\n","    print(f\"Total Trainable Params: {total_params}\")\n","    return total_params"],"metadata":{"id":"rRMAh_i4o6P9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eDPGyBFMrVYn"},"source":["### Making the Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157,"status":"ok","timestamp":1639795397736,"user":{"displayName":"Hex Man","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01194659618639641575"},"user_tz":480},"id":"rz-LJCYo1Mif","outputId":"21701875-edc8-4fd6-8781-fab12bbdf700"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')  \n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qem-bVZeFI2u"},"outputs":[],"source":["def load_reference_net():\n","    net2 = Net().to(device)\n","    state_dict = torch.load(\"best_parameter_project2_part2_model.pth\", map_location=device)\n","    net2.load_state_dict(state_dict)\n","    net2.eval()\n","    return net2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2560,"status":"ok","timestamp":1639795411166,"user":{"displayName":"Hex Man","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01194659618639641575"},"user_tz":480},"id":"nw2vtabYQ_rA","outputId":"f69b337b-d07e-4b37-e6c4-823b61a08438"},"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=800, out_features=500, bias=True)\n","  (fc2): Linear(in_features=500, out_features=5, bias=True)\n",")\n","+--------------+------------+\n","|   Modules    | Parameters |\n","+--------------+------------+\n","| conv1.weight |    500     |\n","|  conv1.bias  |     20     |\n","| conv2.weight |   25000    |\n","|  conv2.bias  |     50     |\n","|  fc1.weight  |   400000   |\n","|   fc1.bias   |    500     |\n","|  fc2.weight  |    2500    |\n","|   fc2.bias   |     5      |\n","+--------------+------------+\n","Total Trainable Params: 428575\n","+--------------+------------+\n","|   Modules    | Parameters |\n","+--------------+------------+\n","| conv1.weight |    500     |\n","| conv2.weight |   25000    |\n","|  fc1.weight  |   400000   |\n","|  fc2.weight  |    2500    |\n","+--------------+------------+\n","Total Trainable Params: 428000\n","428575\n","428000\n"]}],"source":["class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 1 input image channel, 6 output channels, 5x5 square convolution\n","        # kernel\n","        self.conv1 = nn.Conv2d(1, 20, 5)\n","        self.conv2 = nn.Conv2d(20, 50, 5)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(50 * 4 * 4 , 500)  # 28*28 from image dimension \n","        self.fc2 = nn.Linear(500, 5)\n","\n","    def forward(self, x):\n","      # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        # If the size is a square, you can specify with a single number\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","net = Net().to(device)\n","print(net)\n","total_parameters = count_parameters(net)\n","total_parameters_no_bias = count_parameters_no_bias(net)\n","print(total_parameters)\n","print(total_parameters_no_bias)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":123,"status":"ok","timestamp":1639795414840,"user":{"displayName":"Hex Man","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01194659618639641575"},"user_tz":480},"id":"4SFg7JFkIzjX","outputId":"446ae09d-86b7-46e1-aea3-fcb585778c61"},"outputs":[{"output_type":"stream","name":"stdout","text":["8\n","torch.Size([20, 1, 5, 5])\n"]}],"source":["params = list(net.parameters())\n","print(len(params))\n","print(params[0].size())  # conv1's .weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJLjST8LOOEk"},"outputs":[],"source":["# input = torch.randn(1, 1, 28, 28).to(device)\n","# out = net(input)\n","# print(out)"]},{"cell_type":"markdown","metadata":{"id":"8K_C6_DzrYn4"},"source":["### Getting the subset MNIST dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_oj8JgSZs83"},"outputs":[],"source":["def data_loader(batch_size = batchsize, n_workers = num_workers):\n","    train_data_th = datasets.MNIST(root='./datasets', download=True, train=True)\n","    test_data_th = datasets.MNIST(root='./datasets', download=True, train=False)\n","\n","    label = [1, 2, 3 ,4, 6]\n","    data_train_fea = np.array(train_data_th.data[:]).astype(np.float32)\n","    data_train_fea = (data_train_fea / 255)\n","    data_train_gnd = np.array(train_data_th.targets)\n","    ctr1_idx = np.where(data_train_gnd[:] == label[0])\n","    ctr2_idx = np.where(data_train_gnd[:] == label[1])\n","    ctr3_idx = np.where(data_train_gnd[:] == label[2])\n","    ctr4_idx = np.where(data_train_gnd[:] == label[3])\n","    ctr6_idx = np.where(data_train_gnd[:] == label[4])\n","    ctr1_idx = np.array(ctr1_idx)\n","    ctr2_idx = np.array(ctr2_idx)\n","    ctr3_idx = np.array(ctr3_idx)\n","    ctr4_idx = np.array(ctr4_idx)\n","    ctr6_idx = np.array(ctr6_idx)\n","    total_train_valid_idx = np.concatenate((ctr1_idx, ctr2_idx, ctr3_idx, ctr4_idx, ctr6_idx),axis = None)\n","    np.random.shuffle(total_train_valid_idx)#forgot to add this line of code\n","    train_idx = total_train_valid_idx[:27000]\n","    valid_idx = total_train_valid_idx[27000:]\n","\n","    data_total_train = data_train_fea[total_train_valid_idx]\n","    target_total_train = data_train_gnd[total_train_valid_idx]\n","\n","    data_train = data_train_fea[train_idx]\n","    target_train = data_train_gnd[train_idx]\n","\n","    data_validation = data_train_fea[valid_idx]\n","    target_validation = data_train_gnd[valid_idx]\n","\n","    data_test_fea = np.array(test_data_th.data[:]).astype(np.float32)\n","    data_test_fea = (data_test_fea / 255)\n","    data_test_gnd = np.array(test_data_th.targets)\n","    cte1_idx = np.where(data_test_gnd[:] == label[0])\n","    cte2_idx = np.where(data_test_gnd[:] == label[1])\n","    cte3_idx = np.where(data_test_gnd[:] == label[2])\n","    cte4_idx = np.where(data_test_gnd[:] == label[3])\n","    cte6_idx = np.where(data_test_gnd[:] == label[4])\n","    cte1_idx = np.array(cte1_idx)\n","    cte2_idx = np.array(cte2_idx)\n","    cte3_idx = np.array(cte3_idx)\n","    cte4_idx = np.array(cte4_idx)\n","    cte6_idx = np.array(cte6_idx)\n","    test_idx = np.concatenate((cte1_idx, cte2_idx, cte3_idx, cte4_idx, cte6_idx),axis = None)\n","\n","    data_test = data_test_fea[test_idx]\n","    target_test = data_test_gnd[test_idx]\n","\n","    ##not sure what this is doing but it was here in both the neural network tutorial and in yerlan's collab\n","    dtrain_mean = data_train.mean(axis=0)\n","    data_train -= dtrain_mean\n","    data_validation -=dtrain_mean\n","    data_total_train -= dtrain_mean\n","    data_test -= dtrain_mean\n","    ##\n","\n","    #######\n","    #https://discuss.pytorch.org/t/indexerror-target-2-is-out-of-bounds/69614/24\n","    tensor_target_train = torch.from_numpy(target_train)\n","\n","    #print(tensor_target_train.size())\n","    # print(min(tensor_target_train))\n","    # print(max(tensor_target_train))\n","    unique_targets_train = torch.unique(tensor_target_train) #1,2,3,4,6\n","    # print('unique_targets_train: {}'.format(unique_targets_train))\n","\n","    new_tensor_target_train = torch.empty_like(tensor_target_train) #size of tensor_target_train\n","    for idx, t in enumerate(unique_targets_train):\n","        # print('replacing {} with {}'.format(t, idx))\n","        new_tensor_target_train[tensor_target_train == t] = idx # [1,1,3,3]\n","    # print(new_tensor_target_train.size())\n","    # print(min(new_tensor_target_train))\n","    # print(max(new_tensor_target_train))\n","\n","    tensor_target_validation = torch.from_numpy(target_validation)\n","    #print(tensor_target_validation.size())\n","    unique_targets_validation = torch.unique(tensor_target_validation)\n","    new_tensor_target_validation = torch.empty_like(tensor_target_validation)\n","    for idx, t in enumerate(unique_targets_validation):\n","      new_tensor_target_validation[tensor_target_validation == t] = idx\n","\n","    tensor_target_test = torch.from_numpy(target_test)\n","    unique_targets_test = torch.unique(tensor_target_test)\n","    new_tensor_target_test = torch.empty_like(tensor_target_test)\n","    for idx, t in enumerate(unique_targets_test):\n","      new_tensor_target_test[tensor_target_test == t] = idx\n","    \n","\n","    tensor_target_total_train = torch.from_numpy(target_total_train)\n","    unique_targets_total_train = torch.unique(tensor_target_total_train)\n","    new_tensor_target_total_train = torch.empty_like(tensor_target_total_train)\n","    for idx, t in enumerate(unique_targets_total_train):\n","      new_tensor_target_total_train[tensor_target_total_train == t] = idx\n","\n","\n","    train_data = TensorDataset(torch.from_numpy(data_train), new_tensor_target_train)\n","    validation_data = TensorDataset(torch.from_numpy(data_validation), new_tensor_target_validation)\n","    test_data = TensorDataset(torch.from_numpy(data_test), new_tensor_target_test)\n","    total_train_data = TensorDataset(torch.from_numpy(data_total_train), new_tensor_target_total_train)\n","\n","    train_loader = DataLoader(train_data, num_workers=n_workers, batch_size=batch_size, shuffle=True)\n","    validation_loader = DataLoader(validation_data, num_workers = n_workers, batch_size = batch_size, shuffle = True)\n","    test_loader = DataLoader(test_data, num_workers=n_workers, batch_size=batch_size, shuffle=False)\n","    total_train_loader = DataLoader(total_train_data, num_workers = n_workers, batch_size = batch_size, shuffle = True)\n","\n","    #return train_loader, validation_loader, test_loader, total_train_loader #-> use this one if still training model\n","    return total_train_loader, test_loader # use this one if already found optimized model"]},{"cell_type":"markdown","metadata":{"id":"MPhDvBFfrhb7"},"source":["### Defining all the lists used"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yywffSSQoLBl"},"outputs":[],"source":["plotepoch = []\n","plottrainloss = []\n","plotvalidloss = []\n","avg_train_loss = []\n","avg_valid_loss = []\n","plottrainacc = []\n","plotvalidacc = []\n","timetaken = []\n","loss_list = []\n","avg_train_err = []\n","avg_test_err = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ez4PNdY_CnD"},"outputs":[],"source":["def clearList():\n","    plotepoch.clear()\n","    plottrainloss.clear()\n","    plotvalidloss.clear()\n","    avg_train_loss.clear()\n","    avg_valid_loss.clear()\n","    plottrainacc.clear()\n","    plotvalidacc.clear()"]},{"cell_type":"markdown","metadata":{"id":"cUN-nkcBrfB1"},"source":["### Calculating accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6mJjdV6aLt4"},"outputs":[],"source":["def calc_acc(loader, net):\n","    correct_cnt = 0\n","    total_cnt = 0\n","    net.eval()\n","    with torch.no_grad():\n","        for batch_inputs, batch_labels in loader:\n","            batch_inputs = batch_inputs.to(device=device)\n","            batch_labels = batch_labels.to(dtype=torch.long, device=device)\n","            out = net(batch_inputs[:,None, :,:])\n","            _, pred_labels = torch.max(out.data, 1)\n","            total_cnt += batch_labels.size(0)\n","            correct_cnt += (pred_labels == batch_labels).sum().item()\n","    \n","\n","    accuracy = correct_cnt / total_cnt\n","    return accuracy\n","\n","def calc_acc_loss(loader, net):\n","    correct_cnt = 0\n","    total_cnt = 0\n","    loss_list.clear()\n","    net.eval()\n","    with torch.no_grad():\n","        for batch_inputs, batch_labels in loader:\n","            batch_inputs = batch_inputs.to(device=device)\n","            batch_labels = batch_labels.to(dtype=torch.long, device=device)\n","            out = net(batch_inputs[:,None, :,:])\n","            loss = nnloss(out,batch_labels)\n","            loss_list.append(loss.item())\n","            _, pred_labels = torch.max(out.data, 1)\n","            total_cnt += batch_labels.size(0)\n","            correct_cnt += (pred_labels == batch_labels).sum().item()\n","    \n","    calc_loss = np.mean(loss_list)\n","    accuracy = correct_cnt / total_cnt\n","    return accuracy, calc_loss      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nea1fzQ57g0e"},"outputs":[],"source":["def findAccNoTest(net):\n","    train_loader, validation_loader, test_loader, total_train_loader = data_loader()\n","    print(f'Train Accuracy: {100 * calc_acc(train_loader,net):.2f}%')\n","    print(f'Validation Accuracy: {100 * calc_acc(validation_loader,net):.2f}%')\n","\n","def findAccWithTest(net):\n","    train_loader, validation_loader, test_loader, total_train_loader = data_loader()\n","    print(f'Train Accuracy: {100 * calc_acc(train_loader,net):.2f}%')\n","    print(f'Validation Accuracy: {100 * calc_acc(validation_loader,net):.2f}%')\n","    print(f'Test Accuracy: {100 *calc_acc(test_loader,net):.2f}%')\n","\n","def findAccTrainTest(net):\n","    train_loader, test_loader = data_loader()\n","    print(f'Train Accuracy: {100 * calc_acc(train_loader,net):.2f}%')\n","    print(f'Test Accuracy: {100 *calc_acc(test_loader,net):.2f}%')\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-pyKKF6vQR8"},"outputs":[],"source":["def train_test_acc_eval_f(net):\n","    train_loader, test_loader = data_loader()\n","    acc_train, loss_train = calc_acc_loss(train_loader,net)\n","    acc_test, loss_test = calc_acc_loss(test_loader,net)\n","\n","    print(f\"Train err: {100-acc_train*100:.2f}%, train loss: {loss_train}\")\n","    print(f\"TEST ERR: {100-acc_test*100:.2f}%, test loss: {loss_test}\")\n","    avg_train_err.append(100-acc_train*100)\n","    avg_test_err.append(100-acc_test*100)\n","    "]},{"cell_type":"markdown","metadata":{"id":"H08Y8cF2rr10"},"source":["### Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hEht7fEFOds"},"outputs":[],"source":["#https://christianbernecker.medium.com/how-to-create-a-confusion-matrix-in-pytorch-38d06a7f04b7\n","\n","def confusion_matrix1(test_loader):\n","    y_pred = []\n","    y_true = []\n","\n","    for x, target in test_loader:\n","        x = x.to(device=device)[:,None, :,:]\n","        target = target.to(device=device, dtype=torch.long)\n","        out = net(x)\n","        out = (torch.max(torch.exp(out),1)[1]).data.cpu().numpy()\n","        y_pred.extend(out)\n","        target = target.data.cpu().numpy()\n","        y_true.extend(target)\n","\n","    labels = [1,2,3,4,6]\n","    cf_matrix = confusion_matrix(y_true,y_pred)\n","    df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) * 10, index = [i for i in labels], columns = [i for i in labels])\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm, annot=True)"]},{"cell_type":"markdown","metadata":{"id":"FyGn77OJrx2Q"},"source":["### Yerlan's Suggestions for Project 2 Part 1 Part 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPOBUUwuZc55"},"outputs":[],"source":["#select reasonable settings\n","#every 10 epoch 0.9 schedular\n","#have 60k training\n","#10k test\n","#choose lr: 0.01 - 0.1\n","# lr decay: 0.95 every 10 epoch\n","# epoch 100-150\n","#sgd: use momentum\n","# for momentum: default 0.9 okay\n","# 5 - 10 experiments\n","#once found best parameters\n","#use them on entire dataset 60k\n","#split training 55k,5k train/valid\n","#in our case split training 27k,rest train/valid\n","#once found best parameters\n","#train model on all training\n","#report true test error on 10k only once\n","#use large as possible batch size that fits in gpu allocates like 2000 or 2048"]},{"cell_type":"markdown","metadata":{"id":"QpH_7dLlr00n"},"source":["### Training the Netural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sU0jY3FYt9d"},"outputs":[],"source":["# uncomment to test parameters again\n","#if i set num_workers = 4 i get this warning\n","#/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked))\n","#So I will do what they suggest and switch to 2\n","\n","def train_net(net):\n","    params = list(filter(lambda p: p.requires_grad, net.parameters()))\n","    optimizer = optim.SGD(params, lr=0.1, momentum=0.9, weight_decay = 0, nesterov=True)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n","    train_loader, validation_loader, test_loader, total_train_loader = data_loader()\n","    print(\"Before optimizing the model\")\n","    findAccNoTest(train_loader, validation_loader, net)\n","    print(\"Training the Model\")\n","    epochs = 100\n","    clearList()\n","    start = timer()\n","    for epoch in range(epochs):\n","        net.train()\n","        for x, target in train_loader:\n","            x = x.to(device=device)[:,None, :,:]\n","            target = target.to(device=device, dtype=torch.long)\n","            optimizer.zero_grad()\n","            out = net(x)\n","            loss = nnloss(out, target)\n","            loss.backward()\n","            avg_train_loss.append(loss.item())\n","            optimizer.step()\n","        scheduler.step()\n","        train_loss = np.mean(avg_train_loss)\n","        plotepoch.append(epoch)\n","        plottrainloss.append(train_loss)\n","        \n","        #https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n","        net.eval()\n","        with torch.no_grad():\n","            for x, target in validation_loader:\n","                x = x.to(device=device)[:,None, :,:]\n","                target = target.to(device=device, dtype=torch.long)\n","                out = net(x)\n","                loss = nnloss(out, target)\n","                avg_valid_loss.append(loss.item())\n","        valid_loss = np.mean(avg_valid_loss)\n","        plotvalidloss.append(valid_loss)\n","\n","        if epoch % 10 == 0:\n","            print(f\"\\tepoch #{epoch} is finished.\")\n","            print(f\"\\t  Avg. Train loss: {train_loss}\")\n","            print(f\"\\t  Avg. Validation loss: {valid_loss}\")\n","        \n","        train_acc = calc_acc(train_loader, net)\n","        plottrainacc.append(train_acc)\n","        valid_acc = calc_acc(validation_loader, net)\n","        plotvalidacc.append(valid_acc)        \n","\n","    fig = plt.figure(1)\n","    plt.plot(plotepoch,plottrainloss,color = \"blue\", label = \"Average Train Loss\")\n","    plt.plot(plotepoch,plotvalidloss,color = \"red\", label = \"Average Validation Loss\")\n","    plt.title('Epoch vs Train and Validation Average Loss')\n","    plt.legend(loc=\"upper right\")\n","    plt.xlabel(\"Increasing Epoch Value by 1\")\n","    plt.ylabel(\"Avg Loss\")\n","    plt.show()\n","    fig2 = plt.figure(2)\n","    plt.plot(plottrainacc, color=\"blue\", label=\"Train Accuracy\")\n","    plt.plot(plotvalidacc, color=\"red\", label=\"Validation Accuracy\")\n","    plt.title('Epoch vs Train and Validation Accuracy')\n","    plt.legend(loc=\"lower right\")\n","    plt.xlabel(\"Increasing Epoch Value by 1\")\n","    plt.ylabel(\"Accuracy Score\")\n","    plt.show()\n","    print(\"After optimizing the model\")\n","    findAccWithTest(train_loader, validation_loader, test_loader, net)\n","    \n","    end = timer()\n","\n","    #https://stackoverflow.com/questions/7370801/how-to-measure-elapsed-time-in-python\n","    taken=(end-start)\n","    timetaken.append(taken)\n","    if(taken > 60):\n","        timein = \"minutes\"\n","    else:\n","        timein = \"seconds\"\n","    print(f\"It took us {timedelta(seconds=taken)} {timein} to run this loop when running on {device}\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vw4o3gdRiGio"},"outputs":[],"source":["#uncomment this code if we need to train the model\n","#train_net(net)"]},{"cell_type":"markdown","metadata":{"id":"NQBBeGUsr35W"},"source":["### Code to Run the Optimized Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JYfpIoEmg6i"},"outputs":[],"source":["# The code above was for testing and find the best parameters\n","\n","# For training the model you only need the code down below\n","def best_train_net(net):\n","    train_loader, test_loader = data_loader()\n","    print(\"Before optimizing the model\")\n","    print(f'Train Accuracy: {100 * calc_acc(train_loader,net):.2f}%')\n","    params = list(filter(lambda p: p.requires_grad, net.parameters()))\n","    optimizer = optim.SGD(params, lr=0.1, momentum=0.9, weight_decay = 0, nesterov=True)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n","    timein = \"tmp\"\n","    epochs = 100\n","    clearList()\n","    start = timer()        \n","    for epoch in range(epochs):\n","        net.train()\n","        for x, target in train_loader:\n","            x = x.to(device)[:,None,:,:]\n","            target = target.to(dtype=torch.long, device=device)\n","            optimizer.zero_grad()\n","            out = net(x)\n","            loss = nnloss(out, target)\n","            loss.backward()\n","            avg_train_loss.append(loss.item())\n","            optimizer.step()\n","        scheduler.step()\n","        train_loss = np.mean(avg_train_loss)\n","        if epoch % 10 == 0:\n","            print(f\"\\tepoch #{epoch} is finished.\")\n","            print(f\"\\t  Avg. Train loss: {train_loss}\")\n","\n","    end = timer()\n","\n","    #https://stackoverflow.com/questions/7370801/how-to-measure-elapsed-time-in-python\n","    taken=(end-start)\n","    timetaken.append(taken)\n","    if(taken > 60):\n","        timein = \"minutes\"\n","    else:\n","        timein = \"seconds\"\n","    print(\"After optimizing the model\")\n","    findAccTrainTest(net)\n","    print(f\"It took us {timedelta(seconds=taken)} {timein} to run this loop when running on {device}\")\n","    confusion_matrix1(test_loader)\n","    torch.save(net.state_dict(), \"best_parameter_project2_part2_model.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEIEP-jjkMww","colab":{"base_uri":"https://localhost:8080/","height":879},"executionInfo":{"status":"ok","timestamp":1639791693340,"user_tz":480,"elapsed":136823,"user":{"displayName":"Brian Matamet","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11894252957505598565"}},"outputId":"03acb683-a389-4768-9b11-b57fa200acde"},"outputs":[{"output_type":"stream","name":"stdout","text":["Before optimizing the model\n","Train Accuracy: 23.25%\n","\tepoch #0 is finished.\n","\t  Avg. Train loss: 1.383716106414795\n","\tepoch #10 is finished.\n","\t  Avg. Train loss: 0.1786777003075589\n","\tepoch #20 is finished.\n","\t  Avg. Train loss: 0.09839892200682135\n","\tepoch #30 is finished.\n","\t  Avg. Train loss: 0.06826243188071957\n","\tepoch #40 is finished.\n","\t  Avg. Train loss: 0.05227437552558713\n","\tepoch #50 is finished.\n","\t  Avg. Train loss: 0.04234637777873443\n","\tepoch #60 is finished.\n","\t  Avg. Train loss: 0.03558590865714343\n","\tepoch #70 is finished.\n","\t  Avg. Train loss: 0.03068876975440604\n","\tepoch #80 is finished.\n","\t  Avg. Train loss: 0.026980494374218646\n","\tepoch #90 is finished.\n","\t  Avg. Train loss: 0.024075511401568644\n","After optimizing the model\n","Train Accuracy: 100.00%\n","Test Accuracy: 99.69%\n","It took us 0:02:12.991520 minutes to run this loop when running on cuda\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x504 with 2 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["best_train_net(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoWTjGR5x9NK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639795477676,"user_tz":480,"elapsed":1472,"user":{"displayName":"Hex Man","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01194659618639641575"}},"outputId":"1fd66684-077e-49d9-a979-4855df5958b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train err: 0.00%, train loss: 0.00043705475012150905\n","TEST ERR: 0.33%, test loss: 0.012950082619984945\n"]}],"source":["net = load_reference_net()\n","train_test_acc_eval_f(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwt1rzDmqZQW"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"iLos7uzEhq8k"},"source":["Below is for Project 2 Part 1 Part 2 LC Compression\n","\n","All the code was taken from Yerlan's LC Compression Collab and modified to fit the parameters of the project and our model\n","\n","We only need to use 3 weight compression types:\n","AdaptiveQuantization, L0ConstrainedPruning, and RankSelection compressions. \n","\n","For each compression type, you will need to vary the amount of compression and report to us 'compression ratio vs error' plots."]},{"cell_type":"markdown","metadata":{"id":"Ym1nubjvqoM7"},"source":["## Compression using the LC toolkit"]},{"cell_type":"markdown","metadata":{"id":"u_xswZO9iQ50"},"source":["### Step 1: L step\n","We will use same L step with same hyperparamters for all our compression examples"]},{"cell_type":"markdown","source":["The compute compression ratio needs to be edited\n","\n","I can find the total_number_of_parameters using a function that is being called above\n","\n","428000 is what the function finds"],"metadata":{"id":"1UZim_-srUtb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1i6YyE4WiokY"},"outputs":[],"source":["#based on yerlan's code \n","#784x300, 300x100, and 100x10,\n","# compressed_model_bits = lc_alg.count_param_bits() + (300+100+10)*32\n","# uncompressed_model_bits = (784*300+300*100+100*10 + 300 + 100 + 10)*32\n","#i can replace 784*300+300*100+100*10 with total_parameters_no_bias\n","# so uncompressed_model_bits = (total_parameters_no_bias + 500 + 5)*32\n","# for compressed_model_bits = lc_alg.count_param_bits() + (500 +5) * 32\n","def compute_compression_ratio(lc_alg):\n","    compressed_model_bits = lc_alg.count_param_bits() + (500 +5) * 32\n","    uncompressed_model_bits = (total_parameters_no_bias)*32\n","    compression_ratio = uncompressed_model_bits/compressed_model_bits\n","    return compression_ratio\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-vYCfwPiTqX"},"outputs":[],"source":["def my_l_step(model, lc_penalty, step):\n","    train_loader, test_loader = data_loader()\n","    params = list(filter(lambda p: p.requires_grad, model.parameters()))\n","    # lrate = [0.1, 0.01, 0.001, 0.1*(0.98**step), 0.01*(0.98**step) ]\n","    lr = 0.001*(0.99**step)\n","    optimizer = optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\n","    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n","    print(f'L-step #{step} with lr: {lr:.5f}')\n","    epochs_per_step_ = 10\n","    # if step == 0:\n","    #     epochs_per_step_ = epochs_per_step_ * 2\n","    for epoch in range(epochs_per_step_):\n","        avg_loss = []\n","        for x, target in train_loader:\n","            x = x.to(device)[:,None,:,:]\n","            target = target.to(dtype=torch.long, device=device)\n","            optimizer.zero_grad()\n","            out = model(x)\n","            #loss = model.loss(out, target) + lc_penalty()\n","            loss = nnloss(out,target) + lc_penalty()\n","            avg_loss.append(loss.item())\n","            loss.backward()\n","            optimizer.step()\n","        #scheduler.step()\n","        print(f\"\\tepoch #{epoch} is finished.\")\n","        print(f\"\\t  avg. train loss: {np.mean(avg_loss):.6f}\")"]},{"cell_type":"markdown","metadata":{"id":"1lzUNbRdiXKA"},"source":["### Step 2: Schedule of mu values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKr30OY6iZCr"},"outputs":[],"source":["mu_s = [5e-3 * (1.25 ** n) for n in range(15)] # 10e-5 1.4 ** n\n","# 5 L-C steps in total\n","# total training epochs is 3 x 5 = 15"]},{"cell_type":"markdown","metadata":{"id":"Z831i_Dkic5I"},"source":["### Compression time! Pruning\n","Let us prune all but 5% of the weights in the network (5% = 21,400 weights)\n","\n","I am using the function called above to find total parameters, which gives us 428000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BIRvtimidfn","colab":{"base_uri":"https://localhost:8080/","height":710},"executionInfo":{"status":"error","timestamp":1639791703827,"user_tz":480,"elapsed":9375,"user":{"displayName":"Brian Matamet","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11894252957505598565"}},"outputId":"33636ecf-7c49-41a3-9d1e-34f6e7d8aa02"},"outputs":[{"output_type":"stream","name":"stdout","text":["from retrieve 246.0872446651058\n","L0 constrained pruning.\n","l0-cons pruning finished. #zeros: 86.18%\n","Direct compression has been performed.\n","Train err: 0.09%, train loss: 0.003234356672813495\n","TEST ERR: 0.39%, test loss: 0.01540922528753678\n","0.0001\n","L-step #0 with lr: 0.05000\n","\tepoch #0 is finished.\n","\t  avg. train loss: 0.006394\n","\tepoch #1 is finished.\n","\t  avg. train loss: 0.006391\n","\tepoch #2 is finished.\n","\t  avg. train loss: 0.006374\n","\tepoch #3 is finished.\n","\t  avg. train loss: 0.006364\n","\tepoch #4 is finished.\n","\t  avg. train loss: 0.006348\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-a3ecc1aa62fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mevaluation_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_acc_eval_f\u001b[0m \u001b[0;31m# evaluation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   )\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mlc_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                              \u001b[0;31m# entry point to the LC algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mpruning_compression_ratios\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_compression_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc_alg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mpruning_train_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_train_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/LC-model-compression/lc/main.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"L-step #{step_number} has finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/LC-model-compression/lc/main.py\u001b[0m in \u001b[0;36ml_step\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_step_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlc_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-d987bbb4d768>\u001b[0m in \u001b[0;36mmy_l_step_prune\u001b[0;34m(model, lc_penalty, step)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlc_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mavg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["total_weights = total_parameters_no_bias\n","prune_offset = [0.13, 0.10, 0.07, 0.04, 0.03, 0.025]\n","pruning_compression_ratios = []\n","pruning_train_err = []\n","pruning_test_err = []\n","num_parameters_weights = []\n","pruning_num_parameters = []\n","for i in prune_offset:\n","  net = load_reference_net()\n","  avg_train_err.clear()\n","  avg_test_err.clear()\n","  layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n","  compression_tasks = {\n","      Param(layers, device): (AsVector, ConstraintL0Pruning(kappa=int(total_weights*i)), 'pruning')\n","  }\n","\n","  lc_alg = lc.Algorithm(\n","      model=net,                            # model to compress\n","      compression_tasks=compression_tasks,  # specifications of compression\n","      l_step_optimization=my_l_step,        # implementation of L-step\n","      mu_schedule=mu_s,                     # schedule of mu values\n","      evaluation_func=train_test_acc_eval_f # evaluation function\n","  )\n","  lc_alg.run()                              # entry point to the LC algorithm\n","  pruning_compression_ratios.append(compute_compression_ratio(lc_alg))\n","  pruning_train_err.append(avg_train_err[-1])\n","  pruning_test_err.append(avg_test_err[-1])\n","  num_parameters_weights.append(int(total_weights*i))\n","  pruning_num_parameters.append(lc_alg.count_params())\n","  print(f\"Inserting compression ratio: {compute_compression_ratio(lc_alg)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2HG4UQ7iihx"},"outputs":[],"source":["print(f\"Pruning Compression Ratios: {pruning_compression_ratios}\")\n","print(f\"pruning_train_err: {pruning_train_err}\")\n","print(f\"pruning_test_err: {pruning_test_err}\")\n","print(f\"num_parameters_weights: {num_parameters_weights}\")\n","print(f\"pruning_num_parameters: {pruning_num_parameters}\")\n","# These results were found using 30 L-C steps with 10 epochs per step. 6 Different configurations were ran.\n","# Pruning Compression Ratios: [6.65839233167264, 8.50902646256494, 11.872071951951224, 19.984842478371384, 26.019822045621446, 30.832342101890347]\n","# pruning_train_err: [0.0862577123147126, 0.09100294098728103, 0.15448355567365793, 0.35547035233850105, 0.5792342466316784, 0.4917111400042814]\n","# pruning_test_err: [0.37194172492702743, 0.39022360632174563, 0.42867859822098214, 0.6196927383106263, 0.7577524633259126, 0.7136237840972842]\n","# num_parameters_weights: [55640, 42800, 29960, 17120, 12840, 10700]\n","# pruning_num_parameters: [55640, 42800, 29960, 17120, 12840, 10700]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuX9qoh8h8ht"},"outputs":[],"source":["fig = plt.figure(1)\n","plt.plot(pruning_compression_ratios,pruning_train_err,color = \"blue\", label = \"L0Constrained-Pruning_train_err\", marker = \"o\")\n","plt.plot(pruning_compression_ratios,pruning_test_err,color = \"red\", label = \"L0Constrained-Pruning_test_err\", marker = \"o\")\n","plt.title('Compression Ratio vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Compression Ratio\")\n","plt.ylabel(\"Error %\")\n","plt.savefig('pruning-compression.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KANB_6fuLdkl"},"outputs":[],"source":["plt.plot(pruning_num_parameters,pruning_train_err,color = \"blue\", label = \"L0Constrained-Pruning_train_err\", marker = \"o\")\n","plt.plot(pruning_num_parameters,pruning_test_err,color = \"red\", label = \"L0Constrained-Pruning_test_err\", marker = \"o\")\n","plt.title('Number of Parameters vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Number of Parameters\")\n","plt.ylabel(\"Error %\")\n","plt.savefig('pruning-parameters.png')"]},{"cell_type":"markdown","metadata":{"id":"ICRfZbzfimWK"},"source":["Note that we were pruning 95% of the weights. Naively, you would assume 20x compression ratio (100%/5%), however, this is not the case. Firstly, there are some uncompressed parts (in this case biases), and, secondly, storing a compressed model requires additional metadata (in this case positions of non-zero elements). Therefore we get only 16x compression ratio (vs naively expected 20x). \n","\n","To prevent manual computation of compression ratio, let us create a function below. Note, this function is model specific."]},{"cell_type":"markdown","metadata":{"id":"pwVUkcZlisac"},"source":["### Quantization\n","Now let us quantize each layer with its own codebook\n","\n","Looks like k = # of codebooks as on the [github](https://github.com/UCMerced-ML/LC-model-compression/blob/f8a0a3eb1ff18f9796fa15a063925d9d32a1cee1/lc/compression_types/quantization.py)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXfFy3ZqivYb"},"outputs":[],"source":["k_values = [2,4,15, 30] # K_values is also = to size of codebook\n","quant_compression_ratios = []\n","quant_train_err = []\n","quant_test_err = []\n","quant_num_parameters = []\n","for i in k_values:\n","  net = load_reference_net()\n","  avg_train_err.clear()\n","  avg_test_err.clear()\n","  layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n","\n","  compression_tasks = {\n","      Param(layers[0], device): (AsVector, AdaptiveQuantization(k=i), 'layer0_quant'),\n","      Param(layers[1], device): (AsVector, AdaptiveQuantization(k=i), 'layer1_quant')\n","  }\n","\n","  lc_alg = lc.Algorithm(\n","      model=net,                            # model to compress\n","      compression_tasks=compression_tasks,  # specifications of compression\n","      l_step_optimization=my_l_step,        # implementation of L-step\n","      mu_schedule=mu_s,                     # schedule of mu values\n","      evaluation_func=train_test_acc_eval_f # evaluation function\n","  )\n","  lc_alg.run()  \n","  print('Compressed_params:', lc_alg.count_params())\n","  print('Compression_ratio:', compute_compression_ratio(lc_alg))\n","\n","  quant_compression_ratios.append(compute_compression_ratio(lc_alg))\n","  quant_train_err.append(avg_train_err[-1])\n","  quant_test_err.append(avg_test_err[-1])\n","  quant_num_parameters.append(lc_alg.count_params())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ba0s3u2v4y3"},"outputs":[],"source":["print(quant_compression_ratios)\n","print(quant_train_err)\n","print(quant_test_err)\n","print(quant_num_parameters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXpSdWzInQmo"},"outputs":[],"source":["plt.plot(quant_compression_ratios,quant_train_err,color = \"blue\", label = \"Adaptive-Quantization_train_err\", marker = \"o\")\n","plt.plot(quant_compression_ratios,quant_test_err,color = \"red\", label = \"Adaptive-Quantization_test_err\", marker = \"o\")\n","plt.title('Compression Ratio vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Compression Ratio\")\n","plt.ylabel(\"Error %\")  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"muC5UQFoLhSK"},"outputs":[],"source":["plt.plot(quant_num_parameters,quant_train_err,color = \"blue\", label = \"Adaptive-Quantization_train_err\" ,marker = \"o\")\n","plt.plot(quant_num_parameters,quant_test_err,color = \"red\", label = \"Adaptive-Quantization_test_err\",marker = \"o\")\n","plt.title('Size of Codebook vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Size of Codebook\")\n","plt.ylabel(\"Error %\")"]},{"cell_type":"markdown","metadata":{"id":"pOf5GiCEi68m"},"source":["### Low-rank compression with automatic rank selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojI-n6a5i96p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca0346bb-610c-4ce2-f661-344698be3a88","executionInfo":{"status":"ok","timestamp":1639799208843,"user_tz":480,"elapsed":290643,"user":{"displayName":"Hex Man","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01194659618639641575"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Initial train/test errors\n","Train err: 0.00%, train loss: 0.00043602517204514396\n","TEST ERR: 0.33%, test loss: 0.012941837310791016\n","from retrieve 216.08541488785428\n","from retrieve 29.998205942895535\n","for this layer, selected rank is 0, normalized w=1.000, true w=216.085, ()=0.000, w-()=1.000e+00,  should be at least 6.78e-03\n","for this layer, selected rank is 0, normalized w=1.000, true w=29.998, ()=0.000, w-()=1.000e+00,  should be at least 6.37e-04\n","Direct compression has been performed.\n","Train err: 80.65%, train loss: 1.620549464225769\n","TEST ERR: 81.28%, test loss: 1.5911225875218709\n","0.005\n","L-step #0 with lr: 0.00100\n","\tepoch #0 is finished.\n","\t  avg. train loss: 0.615452\n","\tepoch #1 is finished.\n","\t  avg. train loss: 0.614742\n","\tepoch #2 is finished.\n","\t  avg. train loss: 0.613866\n","\tepoch #3 is finished.\n","\t  avg. train loss: 0.612958\n","\tepoch #4 is finished.\n","\t  avg. train loss: 0.612046\n","\tepoch #5 is finished.\n","\t  avg. train loss: 0.611134\n","\tepoch #6 is finished.\n","\t  avg. train loss: 0.610223\n","\tepoch #7 is finished.\n","\t  avg. train loss: 0.609313\n","\tepoch #8 is finished.\n","\t  avg. train loss: 0.608403\n","\tepoch #9 is finished.\n","\t  avg. train loss: 0.607496\n","from retrieve 213.0440327376084\n","from retrieve 29.579754406808245\n","L-step #0 has finished.\n","for this layer, selected rank is 0, normalized w=1.000, true w=213.044, ()=0.000, w-()=1.000e+00,  should be at least 6.78e-03\n","for this layer, selected rank is 4, normalized w=1.000, true w=29.580, ()=0.989, w-()=1.112e-02,  should be at least 6.37e-04\n","C-step #0 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 80.52%, train loss: 1.6248512029647828\n","TEST ERR: 79.83%, test loss: 1.6097362438837688\n","0.00625\n","L-step #1 with lr: 0.00098\n","\tepoch #0 is finished.\n","\t  avg. train loss: 2.160012\n","\tepoch #1 is finished.\n","\t  avg. train loss: 2.156950\n","\tepoch #2 is finished.\n","\t  avg. train loss: 2.153180\n","\tepoch #3 is finished.\n","\t  avg. train loss: 2.149269\n","\tepoch #4 is finished.\n","\t  avg. train loss: 2.145335\n","\tepoch #5 is finished.\n","\t  avg. train loss: 2.141402\n","\tepoch #6 is finished.\n","\t  avg. train loss: 2.137476\n","\tepoch #7 is finished.\n","\t  avg. train loss: 2.133556\n","\tepoch #8 is finished.\n","\t  avg. train loss: 2.129644\n","\tepoch #9 is finished.\n","\t  avg. train loss: 2.125739\n","from retrieve 206.46006417835682\n","from retrieve 29.574077619566072\n","L-step #1 has finished.\n","for this layer, selected rank is 0, normalized w=1.000, true w=678.370, ()=0.000, w-()=1.000e+00,  should be at least 6.78e-03\n","for this layer, selected rank is 5, normalized w=1.000, true w=30.303, ()=1.000, w-()=0.000e+00,  should be at least 6.52e-04\n","C-step #1 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 80.52%, train loss: 1.62485613822937\n","TEST ERR: 79.83%, test loss: 1.6097296078999836\n","0.0078125\n","L-step #2 with lr: 0.00096\n","\tepoch #0 is finished.\n","\t  avg. train loss: 4.840402\n","\tepoch #1 is finished.\n","\t  avg. train loss: 4.831994\n","\tepoch #2 is finished.\n","\t  avg. train loss: 4.821641\n","\tepoch #3 is finished.\n","\t  avg. train loss: 4.810909\n","\tepoch #4 is finished.\n","\t  avg. train loss: 4.800117\n","\tepoch #5 is finished.\n","\t  avg. train loss: 4.789333\n","\tepoch #6 is finished.\n","\t  avg. train loss: 4.778568\n","\tepoch #7 is finished.\n","\t  avg. train loss: 4.767828\n","\tepoch #8 is finished.\n","\t  avg. train loss: 4.757113\n","\tepoch #9 is finished.\n","\t  avg. train loss: 4.746421\n","from retrieve 195.88323445732107\n","from retrieve 29.584264214422053\n","L-step #2 has finished.\n","for this layer, selected rank is 1, normalized w=1.000, true w=1213.287, ()=0.077, w-()=9.226e-01,  should be at least 6.78e-03\n","for this layer, selected rank is 4, normalized w=1.000, true w=29.584, ()=0.989, w-()=1.096e-02,  should be at least 6.37e-04\n","C-step #2 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 80.52%, train loss: 19.678060150146486\n","TEST ERR: 79.83%, test loss: 24.473441243171692\n","0.009765625\n","L-step #3 with lr: 0.00094\n","\tepoch #0 is finished.\n","\t  avg. train loss: 8.059228\n","\tepoch #1 is finished.\n","\t  avg. train loss: 8.042078\n","\tepoch #2 is finished.\n","\t  avg. train loss: 8.020976\n","\tepoch #3 is finished.\n","\t  avg. train loss: 7.999112\n","\tepoch #4 is finished.\n","\t  avg. train loss: 7.977140\n","\tepoch #5 is finished.\n","\t  avg. train loss: 7.955197\n","\tepoch #6 is finished.\n","\t  avg. train loss: 7.933308\n","\tepoch #7 is finished.\n","\t  avg. train loss: 7.911481\n","\tepoch #8 is finished.\n","\t  avg. train loss: 7.889716\n","\tepoch #9 is finished.\n","\t  avg. train loss: 7.868012\n","from retrieve 182.73308749519308\n","from retrieve 29.574365696424916\n","L-step #3 has finished.\n","for this layer, selected rank is 0, normalized w=1.000, true w=1590.848, ()=0.000, w-()=1.000e+00,  should be at least 1.31e-02\n","for this layer, selected rank is 5, normalized w=1.000, true w=30.289, ()=1.000, w-()=0.000e+00,  should be at least 6.52e-04\n","C-step #3 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 80.52%, train loss: 1.624868639310201\n","TEST ERR: 79.83%, test loss: 1.6096549431482952\n","0.01220703125\n","L-step #4 with lr: 0.00092\n","\tepoch #0 is finished.\n","\t  avg. train loss: 12.484604\n","\tepoch #1 is finished.\n","\t  avg. train loss: 12.452062\n","\tepoch #2 is finished.\n","\t  avg. train loss: 12.412035\n","\tepoch #3 is finished.\n","\t  avg. train loss: 12.370579\n","\tepoch #4 is finished.\n","\t  avg. train loss: 12.328940\n","\tepoch #5 is finished.\n","\t  avg. train loss: 12.287376\n","\tepoch #6 is finished.\n","\t  avg. train loss: 12.245940\n","\tepoch #7 is finished.\n","\t  avg. train loss: 12.204642\n","\tepoch #8 is finished.\n","\t  avg. train loss: 12.163485\n","\tepoch #9 is finished.\n","\t  avg. train loss: 12.122465\n","from retrieve 164.03912739032663\n","from retrieve 29.588283939467445\n","L-step #4 has finished.\n","for this layer, selected rank is 0, normalized w=1.000, true w=1982.242, ()=0.000, w-()=1.000e+00,  should be at least 1.33e-02\n","for this layer, selected rank is 4, normalized w=1.000, true w=29.588, ()=0.989, w-()=1.073e-02,  should be at least 6.37e-04\n","C-step #4 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 80.52%, train loss: 1.6248787959416708\n","TEST ERR: 79.83%, test loss: 1.6095305283864338\n","0.0152587890625\n","L-step #5 with lr: 0.00090\n","\tepoch #0 is finished.\n","\t  avg. train loss: 17.809001\n","\tepoch #1 is finished.\n","\t  avg. train loss: 17.752146\n","\tepoch #2 is finished.\n","\t  avg. train loss: 17.682251\n","\tepoch #3 is finished.\n","\t  avg. train loss: 17.609911\n","\tepoch #4 is finished.\n","\t  avg. train loss: 17.537307\n","\tepoch #5 is finished.\n","\t  avg. train loss: 17.464890\n","\tepoch #6 is finished.\n","\t  avg. train loss: 17.392752\n","\tepoch #7 is finished.\n","\t  avg. train loss: 17.320906\n","\tepoch #8 is finished.\n","\t  avg. train loss: 17.249361\n","\tepoch #9 is finished.\n","\t  avg. train loss: 17.178110\n","from retrieve 141.04879178373682\n","from retrieve 29.573476343099088\n","L-step #5 has finished.\n","for this layer, selected rank is 2, normalized w=1.000, true w=2245.484, ()=0.077, w-()=9.226e-01,  should be at least 1.35e-02\n","for this layer, selected rank is 5, normalized w=1.000, true w=30.267, ()=1.000, w-()=0.000e+00,  should be at least 6.51e-04\n","C-step #5 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 57.20%, train loss: 17.012558110555013\n","TEST ERR: 56.62%, test loss: 22.754576206207275\n","0.019073486328125\n","L-step #6 with lr: 0.00089\n","\tepoch #0 is finished.\n","\t  avg. train loss: 22.254145\n","\tepoch #1 is finished.\n","\t  avg. train loss: 22.167179\n","\tepoch #2 is finished.\n","\t  avg. train loss: 22.060382\n","\tepoch #3 is finished.\n","\t  avg. train loss: 21.949991\n","\tepoch #4 is finished.\n","\t  avg. train loss: 21.839342\n","\tepoch #5 is finished.\n","\t  avg. train loss: 21.729109\n","\tepoch #6 is finished.\n","\t  avg. train loss: 21.619434\n","\tepoch #7 is finished.\n","\t  avg. train loss: 21.510328\n","\tepoch #8 is finished.\n","\t  avg. train loss: 21.401798\n","\tepoch #9 is finished.\n","\t  avg. train loss: 21.293842\n","from retrieve 119.19224257278425\n","from retrieve 29.593909320377026\n","L-step #6 has finished.\n","for this layer, selected rank is 1, normalized w=1.000, true w=2168.885, ()=0.037, w-()=9.631e-01,  should be at least 1.42e-02\n","for this layer, selected rank is 4, normalized w=1.000, true w=29.594, ()=0.990, w-()=1.039e-02,  should be at least 6.37e-04\n","C-step #6 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 61.14%, train loss: 10.824617195129395\n","TEST ERR: 60.58%, test loss: 12.788062731424967\n","0.02384185791015625\n","L-step #7 with lr: 0.00087\n","\tepoch #0 is finished.\n","\t  avg. train loss: 26.729001\n","\tepoch #1 is finished.\n","\t  avg. train loss: 26.600947\n","\tepoch #2 is finished.\n","\t  avg. train loss: 26.443812\n","\tepoch #3 is finished.\n","\t  avg. train loss: 26.281546\n","\tepoch #4 is finished.\n","\t  avg. train loss: 26.119054\n","\tepoch #5 is finished.\n","\t  avg. train loss: 25.957341\n","\tepoch #6 is finished.\n","\t  avg. train loss: 25.796609\n","\tepoch #7 is finished.\n","\t  avg. train loss: 25.636890\n","\tepoch #8 is finished.\n","\t  avg. train loss: 25.478192\n","\tepoch #9 is finished.\n","\t  avg. train loss: 25.320499\n","from retrieve 93.65603143503058\n","from retrieve 29.572937915646506\n","L-step #7 has finished.\n","for this layer, selected rank is 2, normalized w=1.000, true w=2073.819, ()=0.051, w-()=9.490e-01,  should be at least 1.94e-02\n","for this layer, selected rank is 5, normalized w=1.000, true w=30.236, ()=1.000, w-()=0.000e+00,  should be at least 6.51e-04\n","C-step #7 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 54.69%, train loss: 6.696203804016113\n","TEST ERR: 55.60%, test loss: 6.201042890548706\n","0.029802322387695312\n","L-step #8 with lr: 0.00085\n","\tepoch #0 is finished.\n","\t  avg. train loss: 30.529851\n","\tepoch #1 is finished.\n","\t  avg. train loss: 30.350765\n","\tepoch #2 is finished.\n","\t  avg. train loss: 30.131329\n","\tepoch #3 is finished.\n","\t  avg. train loss: 29.905038\n","\tepoch #4 is finished.\n","\t  avg. train loss: 29.678806\n","\tepoch #5 is finished.\n","\t  avg. train loss: 29.454015\n","\tepoch #6 is finished.\n","\t  avg. train loss: 29.230937\n","\tepoch #7 is finished.\n","\t  avg. train loss: 29.009566\n","\tepoch #8 is finished.\n","\t  avg. train loss: 28.789940\n","\tepoch #9 is finished.\n","\t  avg. train loss: 28.572061\n","from retrieve 68.61473095685346\n","from retrieve 29.60007985165191\n","L-step #8 has finished.\n","for this layer, selected rank is 1, normalized w=1.000, true w=1849.341, ()=0.040, w-()=9.600e-01,  should be at least 1.31e-02\n","for this layer, selected rank is 5, normalized w=1.000, true w=29.600, ()=1.000, w-()=0.000e+00,  should be at least 6.37e-04\n","C-step #8 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 80.52%, train loss: 17.338049189249674\n","TEST ERR: 79.83%, test loss: 21.49940147002538\n","0.03725290298461914\n","L-step #9 with lr: 0.00083\n","\tepoch #0 is finished.\n","\t  avg. train loss: 31.675916\n","\tepoch #1 is finished.\n","\t  avg. train loss: 31.448628\n","\tepoch #2 is finished.\n","\t  avg. train loss: 31.170446\n","\tepoch #3 is finished.\n","\t  avg. train loss: 30.884006\n","\tepoch #4 is finished.\n","\t  avg. train loss: 30.598048\n","\tepoch #5 is finished.\n","\t  avg. train loss: 30.314369\n","\tepoch #6 is finished.\n","\t  avg. train loss: 30.033289\n","\tepoch #7 is finished.\n","\t  avg. train loss: 29.754858\n","\tepoch #8 is finished.\n","\t  avg. train loss: 29.479059\n","\tepoch #9 is finished.\n","\t  avg. train loss: 29.205863\n","from retrieve 47.50935577228644\n","from retrieve 29.61148457230736\n","L-step #9 has finished.\n","for this layer, selected rank is 3, normalized w=1.000, true w=1546.352, ()=0.056, w-()=9.439e-01,  should be at least 2.09e-02\n","for this layer, selected rank is 5, normalized w=1.000, true w=29.611, ()=1.000, w-()=0.000e+00,  should be at least 6.37e-04\n","C-step #9 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 26.71%, train loss: 5.264779249827067\n","TEST ERR: 26.23%, test loss: 4.268304941554864\n","0.046566128730773926\n","L-step #10 with lr: 0.00082\n","\tepoch #0 is finished.\n","\t  avg. train loss: 31.393857\n","\tepoch #1 is finished.\n","\t  avg. train loss: 31.118006\n","\tepoch #2 is finished.\n","\t  avg. train loss: 30.780951\n","\tepoch #3 is finished.\n","\t  avg. train loss: 30.434625\n","\tepoch #4 is finished.\n","\t  avg. train loss: 30.089677\n","\tepoch #5 is finished.\n","\t  avg. train loss: 29.748254\n","\tepoch #6 is finished.\n","\t  avg. train loss: 29.410680\n","\tepoch #7 is finished.\n","\t  avg. train loss: 29.077046\n","\tepoch #8 is finished.\n","\t  avg. train loss: 28.747322\n","\tepoch #9 is finished.\n","\t  avg. train loss: 28.421372\n","from retrieve 31.346242410446678\n","from retrieve 29.62320546332128\n","L-step #10 has finished.\n","for this layer, selected rank is 2, normalized w=1.000, true w=1179.062, ()=0.044, w-()=9.562e-01,  should be at least 1.72e-02\n","for this layer, selected rank is 5, normalized w=1.000, true w=29.623, ()=1.000, w-()=0.000e+00,  should be at least 6.37e-04\n","C-step #10 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 83.28%, train loss: 32.77698313395182\n","TEST ERR: 82.47%, test loss: 34.74712244669596\n","0.05820766091346741\n","L-step #11 with lr: 0.00080\n","\tepoch #0 is finished.\n","\t  avg. train loss: 27.617376\n","\tepoch #1 is finished.\n","\t  avg. train loss: 27.320291\n","\tepoch #2 is finished.\n","\t  avg. train loss: 26.958012\n","\tepoch #3 is finished.\n","\t  avg. train loss: 26.586637\n","\tepoch #4 is finished.\n","\t  avg. train loss: 26.217704\n","\tepoch #5 is finished.\n","\t  avg. train loss: 25.853428\n","\tepoch #6 is finished.\n","\t  avg. train loss: 25.494186\n","\tepoch #7 is finished.\n","\t  avg. train loss: 25.140031\n","\tepoch #8 is finished.\n","\t  avg. train loss: 24.790951\n","\tepoch #9 is finished.\n","\t  avg. train loss: 24.446780\n","from retrieve 21.817821664372048\n","from retrieve 29.636696806616744\n","L-step #11 has finished.\n","for this layer, selected rank is 7, normalized w=1.000, true w=826.865, ()=0.094, w-()=9.055e-01,  should be at least 2.90e-02\n","for this layer, selected rank is 5, normalized w=1.000, true w=29.637, ()=1.000, w-()=0.000e+00,  should be at least 6.37e-04\n","C-step #11 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 17.15%, train loss: 1.4826897859573365\n","TEST ERR: 15.30%, test loss: 1.1977391044298809\n","0.07275957614183426\n","L-step #12 with lr: 0.00078\n","\tepoch #0 is finished.\n","\t  avg. train loss: 20.588140\n","\tepoch #1 is finished.\n","\t  avg. train loss: 20.316546\n","\tepoch #2 is finished.\n","\t  avg. train loss: 19.986234\n","\tepoch #3 is finished.\n","\t  avg. train loss: 19.648685\n","\tepoch #4 is finished.\n","\t  avg. train loss: 19.314315\n","\tepoch #5 is finished.\n","\t  avg. train loss: 18.985254\n","\tepoch #6 is finished.\n","\t  avg. train loss: 18.661806\n","\tepoch #7 is finished.\n","\t  avg. train loss: 18.343918\n","\tepoch #8 is finished.\n","\t  avg. train loss: 18.031532\n","\tepoch #9 is finished.\n","\t  avg. train loss: 17.724550\n","from retrieve 22.786184699234376\n","from retrieve 29.65364449296198\n","L-step #12 has finished.\n","for this layer, selected rank is 3, normalized w=1.000, true w=474.548, ()=0.041, w-()=9.595e-01,  should be at least 2.21e-02\n","for this layer, selected rank is 5, normalized w=1.000, true w=29.654, ()=1.000, w-()=0.000e+00,  should be at least 6.37e-04\n","C-step #12 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 44.40%, train loss: 5.702928256988526\n","TEST ERR: 43.37%, test loss: 7.239727318286896\n","0.09094947017729282\n","L-step #13 with lr: 0.00077\n","\tepoch #0 is finished.\n","\t  avg. train loss: 13.324305\n","\tepoch #1 is finished.\n","\t  avg. train loss: 13.110106\n","\tepoch #2 is finished.\n","\t  avg. train loss: 12.850231\n","\tepoch #3 is finished.\n","\t  avg. train loss: 12.585528\n","\tepoch #4 is finished.\n","\t  avg. train loss: 12.324282\n","\tepoch #5 is finished.\n","\t  avg. train loss: 12.068205\n","\tepoch #6 is finished.\n","\t  avg. train loss: 11.817342\n","\tepoch #7 is finished.\n","\t  avg. train loss: 11.571792\n","\tepoch #8 is finished.\n","\t  avg. train loss: 11.331455\n","\tepoch #9 is finished.\n","\t  avg. train loss: 11.096156\n","from retrieve 24.858915175089628\n","from retrieve 29.671538372520207\n","L-step #13 has finished.\n","for this layer, selected rank is 9, normalized w=1.000, true w=252.487, ()=0.159, w-()=8.410e-01,  should be at least 1.18e-02\n","for this layer, selected rank is 5, normalized w=1.000, true w=29.672, ()=1.000, w-()=0.000e+00,  should be at least 6.37e-04\n","C-step #13 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 12.26%, train loss: 0.7196536262830099\n","TEST ERR: 11.92%, test loss: 0.6320023387670517\n","0.11368683772161603\n","L-step #14 with lr: 0.00075\n","\tepoch #0 is finished.\n","\t  avg. train loss: 5.740715\n","\tepoch #1 is finished.\n","\t  avg. train loss: 5.626985\n","\tepoch #2 is finished.\n","\t  avg. train loss: 5.489838\n","\tepoch #3 is finished.\n","\t  avg. train loss: 5.350901\n","\tepoch #4 is finished.\n","\t  avg. train loss: 5.214461\n","\tepoch #5 is finished.\n","\t  avg. train loss: 5.081400\n","\tepoch #6 is finished.\n","\t  avg. train loss: 4.951730\n","\tepoch #7 is finished.\n","\t  avg. train loss: 4.825402\n","\tepoch #8 is finished.\n","\t  avg. train loss: 4.702340\n","\tepoch #9 is finished.\n","\t  avg. train loss: 4.582444\n","from retrieve 31.96640584970579\n","from retrieve 29.689030891856238\n","L-step #14 has finished.\n","for this layer, selected rank is 13, normalized w=1.000, true w=91.895, ()=0.262, w-()=7.384e-01,  should be at least 4.28e-03\n","for this layer, selected rank is 5, normalized w=1.000, true w=29.689, ()=1.000, w-()=0.000e+00,  should be at least 6.38e-04\n","C-step #14 has finished.\n","Lagrange multipliers have been updated.\n","Train err: 15.89%, train loss: 0.4004071513811747\n","TEST ERR: 14.23%, test loss: 0.42826353137691814\n","Compressed_params: 19425\n","Compression_ratio: 21.475163070747616\n"]}],"source":["alpha_values = [6.62e-8, 9.62e-8, 1.92e-7, 2.12e-7] #compression numbere 5 , 12.9, 20, 26\n","ranksel_compression_ratios = []\n","ranksel_train_err = []\n","ranksel_test_err = []\n","ranksel_num_parameters = []\n","for i in alpha_values:\n","  net = load_reference_net()\n","  print(f\"Initial train/test errors\")\n","  train_test_acc_eval_f(net)\n","  avg_train_err.clear()\n","  avg_test_err.clear()\n","  layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n","  compression_tasks = {\n","      Param(layers[0], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=i, criterion='storage', module=layers[0], normalize=True), \"layer1_lr\"),\n","      Param(layers[1], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=i, criterion='storage', module=layers[1], normalize=True), \"layer2_lr\")\n","  }\n","\n","  lc_alg = lc.Algorithm(\n","      model=net,                            # model to compress\n","      compression_tasks=compression_tasks,  # specifications of compression\n","      l_step_optimization=my_l_step,        # implementation of L-step\n","      mu_schedule=mu_s,                     # schedule of mu values\n","      evaluation_func=train_test_acc_eval_f # evaluation function\n","  )\n","  lc_alg.run()\n","  print('Compressed_params:', lc_alg.count_params())\n","  print('Compression_ratio:', compute_compression_ratio(lc_alg))\n","\n","  ranksel_compression_ratios.append(compute_compression_ratio(lc_alg))\n","  # ranksel_train_err.append(avg_train_err)\n","  # ranksel_test_err.append(avg_test_err)\n","  ranksel_train_err.append(avg_train_err[-1])\n","  ranksel_test_err.append(avg_test_err[-1])\n","  ranksel_num_parameters.append(lc_alg.count_params())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3TFGsMpvyOf","executionInfo":{"status":"ok","timestamp":1639794350483,"user_tz":480,"elapsed":362,"user":{"displayName":"Hex Man","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01194659618639641575"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a22532e3-b3b4-4c37-8224-a473dc38c27a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","[]\n","[]\n","[]\n"]}],"source":["print(ranksel_compression_ratios)\n","print(ranksel_train_err)\n","print(ranksel_test_err)\n","print(ranksel_num_parameters)\n","# alpha_values = [6.62e-8, 9.62e-8, 1.92e-7, 2.12e-7] #compression numbere 5 , 12.9, 20, 26\n","\n","# 5x compression\n","# alpha = 6.62e-8\n","# Train err: 0.00%, train loss: 0.0006606075059001644\n","# TEST ERR: 0.33%, test loss: 0.01402194673816363\n","\n","# 12x compression\n","# alpha = 1.72e-7\n","# Train err: 3.80%, train loss: 0.12177502165238062\n","# TEST ERR: 3.77%, test loss: 0.1069244456787904\n","# Compressed_params: 33725\n","# Compression_ratio: 12.503651767455448\n","\n","# 20x\n","# alpha = 1.92e-7\n","# Train err: 2.86%, train loss: 0.09001585940519968\n","# TEST ERR: 2.48%, test loss: 0.07083041220903397\n","# Compressed_params: 20725\n","# Compression_ratio: 20.160150730098916\n","\n","# 26x compression\n","# alpha = 2.12e-7\n","# Train err: 4.59%, train loss: 0.1115847259759903\n","# TEST ERR: 5.02%, test loss: 0.11344242530564468\n","# Compressed_params: 15525\n","# Compression_ratio: 26.699937616968185\n","# [26.699937616968185]\n","# [57.41006341734497]\n","# [57.070793433652526]"]},{"cell_type":"code","source":["print(sum(ranksel_train_err[0])/len(ranksel_train_err[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0cQKqqETFQG","executionInfo":{"status":"ok","timestamp":1639793505311,"user_tz":480,"elapsed":125,"user":{"displayName":"Hex Man","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01194659618639641575"}},"outputId":"3334b4f6-382e-4dd2-b654-ecbbfa883c2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["23.43025199273242\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"GjJAMvHaKu82"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZfjneRInaBG"},"outputs":[],"source":["plt.plot(ranksel_compression_ratios,ranksel_train_err,color = \"blue\", label = \"Rank-Selection-compression_train_err\", marker = \"o\")\n","plt.plot(ranksel_compression_ratios,ranksel_test_err,color = \"red\", label = \"Rank-Selection-compression_test_err\", marker = \"o\")\n","plt.title('Compression Ratio vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Compression Ratio\")\n","plt.ylabel(\"Error %\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QG5Ct6FERezr"},"outputs":[],"source":["plt.plot(quant_compression_ratios,quant_test_err,color = \"blue\", label = \"Adaptive-Quantization_test_err\", marker = 'o')\n","plt.plot(pruning_compression_ratios,pruning_test_err,color = \"red\", label = \"L0Constrained-Pruning_test_err\", marker = 'o')\n","plt.plot(ranksel_compression_ratios,ranksel_test_err,color = \"green\", label = \"Rank-Selection-compression_test_err\", marker = 'o')\n","plt.title('Compression Ratio vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Compression Ratio\")\n","plt.ylabel(\"Error %\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAYDmOT5nrQ7"},"outputs":[],"source":["plt.plot(quant_num_parameters,quant_test_err,color = \"blue\", label = \"Adaptive-Quantization_test_err\",marker = \"o\")\n","plt.plot(pruning_num_parameters,pruning_test_err,color = \"red\", label = \"L0Constrained-Pruning_test_err\", marker = \"o\")\n","plt.plot(ranksel_num_parameters,ranksel_test_err,color = \"green\", label = \"Rank-Selection-compression_test_err\",marker = \"o\")\n","plt.title('Number of Parameters / Size of Codebook vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Number of Parameters / Size of Codebook\")\n","plt.ylabel(\"Error %\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"V9qTcP5PeR_o"},"source":["## Extra Code Down Below"]},{"cell_type":"markdown","source":["If you want to use them need to edit it for 2 layers for the fully connected. \n","\n","For conv2d im not sure they are \"layers\""],"metadata":{"id":"TIs9OWjQQR4u"}},{"cell_type":"code","source":["total_weights = total_parameters_no_bias\n","prune_offset = [0.13, 0.10, 0.07, 0.04, 0.03, 0.025]\n","k_values = [30,15,4,2] \n","alpha_values = [6.62e-8, 1.12e-7, 1.92e-7, 2.12e-7] #compression numbere 7 , 11, 15, 21\n","mix_compression_ratios = []\n","mix_train_err = []\n","mix_test_err = []\n","mix_num_parameters = []\n","for i in range(len(k_values)):\n","  net = load_reference_net()\n","  print(f\"Initial train/test errors\")\n","  train_test_acc_eval_f(net)\n","  avg_train_err.clear()\n","  avg_test_err.clear()\n","  layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n","  compression_tasks = {\n","      Param(layers[0], device): (AsVector, AdaptiveQuantization(k=k_values[i]), 'layer0_quant'),\n","      Param(layers[1], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha_values[i], criterion='storage', module=layers[1], normalize=True), \"layer2_lr\")\n","  }\n","\n","  lc_alg = lc.Algorithm(\n","      model=net,                            # model to compress\n","      compression_tasks=compression_tasks,  # specifications of compression\n","      l_step_optimization=my_l_step,        # implementation of L-step\n","      mu_schedule=mu_s,                     # schedule of mu values\n","      evaluation_func=train_test_acc_eval_f # evaluation function\n","  )\n","  lc_alg.run()\n","  print('Compressed_params:', lc_alg.count_params())\n","  print('Compression_ratio:', compute_compression_ratio(lc_alg))\n","  mix_compression_ratios.append(compute_compression_ratio(lc_alg))\n","  mix_train_err.append(avg_train_err[-1])\n","  mix_test_err.append(avg_test_err[-1])\n","  mix_num_parameters.append(lc_alg.count_params())"],"metadata":{"id":"Viiq9TxQVAar"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(mix_compression_ratios)\n","print(mix_train_err)\n","print(mix_test_err)\n","print(mix_num_parameters)"],"metadata":{"id":"yfE0RAV0VBPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(mix_compression_ratios,mix_train_err,color = \"blue\", label = \"Mixture-Of-compression_train_err\", marker = \"o\")\n","plt.plot(mix_compression_ratios,mix_test_err,color = \"red\", label = \"Mixture-Of-compression_test_err\", marker = \"o\")\n","plt.title('Compression Ratio vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Compression Ratio\")\n","plt.ylabel(\"Error %\")"],"metadata":{"id":"UccBhdSxVE1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(mix_num_parameters,mix_train_err,color = \"blue\", label = \"Mixture-Of-compression_train_err\",marker = \"o\")\n","plt.plot(mix_num_parameters,mix_test_err,color = \"red\", label = \"Mixture-Of-compression_test_err\", marker = \"o\")\n","plt.title('Number of Parameters vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Number of Parameters\")\n","plt.ylabel(\"Error %\")"],"metadata":{"id":"AuMyQN7QVJMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_weights = total_parameters_no_bias\n","prune_offset = [0.13, 0.10, 0.07, 0.04, 0.03, 0.025]\n","k_values = [30,15,4,2] \n","#alpha_values = [6.62e-8, 1.12e-7, 1.92e-7, 2.12e-7] #compression numbere 7 , 11, 15, 21\n","mix2_compression_ratios = []\n","mix2_train_err = []\n","mix2_test_err = []\n","mix2_num_parameters = []\n","for i in range(len(k_values)):\n","  net = load_reference_net()\n","  print(f\"Initial train/test errors\")\n","  train_test_acc_eval_f(net)\n","  avg_train_err.clear()\n","  avg_test_err.clear()\n","  layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n","  compression_tasks = {\n","      Param(layers[0], device): (AsVector, ConstraintL0Pruning(kappa=int(total_weights*i)), 'pruning'),\n","      Param(layers[1], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=i, criterion='storage', module=layers[1], normalize=True), \"layer2_lr\")\n","  }\n","\n","  lc_alg = lc.Algorithm(\n","      model=net,                            # model to compress\n","      compression_tasks=compression_tasks,  # specifications of compression\n","      l_step_optimization=my_l_step,        # implementation of L-step\n","      mu_schedule=mu_s,                     # schedule of mu values\n","      evaluation_func=train_test_acc_eval_f # evaluation function\n","  )\n","  lc_alg.run()\n","  print('Compressed_params:', lc_alg.count_params())\n","  print('Compression_ratio:', compute_compression_ratio(lc_alg))\n","  mix2_compression_ratios.append(compute_compression_ratio(lc_alg))\n","  mix2_train_err.append(avg_train_err[-1])\n","  mix2_test_err.append(avg_test_err[-1])\n","  mix2_num_parameters.append(lc_alg.count_params())"],"metadata":{"id":"0Gp2kMRaVQsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(mix2_compression_ratios)\n","print(mix2_train_err)\n","print(mix2_test_err)\n","print(mix2_num_parameters)"],"metadata":{"id":"54x7V4PQVT-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(mix2_compression_ratios,mix2_train_err,color = \"blue\", label = \"Mixture-Of-compression_train_err\", marker = \"o\")\n","plt.plot(mix2_compression_ratios,mix2_test_err,color = \"red\", label = \"Mixture-Of-compression_test_err\", marker = \"o\")\n","plt.title('Compression Ratio vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Compression Ratio\")\n","plt.ylabel(\"Error %\")"],"metadata":{"id":"lrSjOz1vVWCb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(mix2_num_parameters,mix2_train_err,color = \"blue\", label = \"Mixture-Of-compression_train_err\",marker = \"o\")\n","plt.plot(mix2_num_parameters,mix2_test_err,color = \"red\", label = \"Mixture-Of-compression_test_err\", marker = \"o\")\n","plt.title('Number of Parameters vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Number of Parameters\")\n","plt.ylabel(\"Error %\")"],"metadata":{"id":"LyZmYCDaVYrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_weights = total_parameters_no_bias\n","prune_offset = [0.13, 0.10, 0.07, 0.04, 0.03, 0.025]\n","k_values = [30,15,4,2] \n","#alpha_values = [6.62e-8, 1.12e-7, 1.92e-7, 2.12e-7] #compression numbere 7 , 11, 15, 21\n","mix3_compression_ratios = []\n","mix3_train_err = []\n","mix3_test_err = []\n","mix3_num_parameters = []\n","for i in range(len(k_values)):\n","  net = load_reference_net()\n","  print(f\"Initial train/test errors\")\n","  train_test_acc_eval_f(net)\n","  avg_train_err.clear()\n","  avg_test_err.clear()\n","  layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n","  compression_tasks = {\n","      Param(layers, device): [\n","          (AsVector, ConstraintL0Pruning(kappa=int(total_weights*prune_offset[i])), 'pruning'),\n","          (AsVector, AdaptiveQuantization(k=k_values[i]), 'quant')\n","      ]\n","  }\n","\n","  lc_alg = lc.Algorithm(\n","      model=net,                            # model to compress\n","      compression_tasks=compression_tasks,  # specifications of compression\n","      l_step_optimization=my_l_step,        # implementation of L-step\n","      mu_schedule=mu_s,                     # schedule of mu values\n","      evaluation_func=train_test_acc_eval_f # evaluation function\n","  )\n","  lc_alg.run()\n","  print('Compressed_params:', lc_alg.count_params())\n","  print('Compression_ratio:', compute_compression_ratio(lc_alg))\n","  mix3_compression_ratios.append(compute_compression_ratio(lc_alg))\n","  mix3_train_err.append(avg_train_err[-1])\n","  mix3_test_err.append(avg_test_err[-1])\n","  mix3_num_parameters.append(lc_alg.count_params())"],"metadata":{"id":"2FMmPbobVaXw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(mix3_compression_ratios)\n","print(mix3_train_err)\n","print(mix3_test_err)\n","print(mix3_num_parameters)"],"metadata":{"id":"Ap2H81BpVfH2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(mix3_compression_ratios,mix3_train_err,color = \"blue\", label = \"Mixture-Of-compression_train_err\", marker = \"o\")\n","plt.plot(mix3_compression_ratios,mix3_test_err,color = \"red\", label = \"Mixture-Of-compression_test_err\", marker = \"o\")\n","plt.title('Compression Ratio vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Compression Ratio\")\n","plt.ylabel(\"Error %\")"],"metadata":{"id":"SsEWj96KVgj4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(mix3_num_parameters,mix3_train_err,color = \"blue\", label = \"Mixture-Of-compression_train_err\",marker = \"o\")\n","plt.plot(mix3_num_parameters,mix3_test_err,color = \"red\", label = \"Mixture-Of-compression_test_err\", marker = \"o\")\n","plt.title('Number of Parameters vs Train and Test Error')\n","plt.legend(loc=\"best\")\n","plt.xlabel(\"Number of Parameters\")\n","plt.ylabel(\"Error %\")"],"metadata":{"id":"jqU5aUlOViTY"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Project2_Part2_GroupF_Neural_Network.ipynb","provenance":[{"file_id":"1FQJLgrrT7TXZkQfVJsmrQmiAl7srWACu","timestamp":1639184828985},{"file_id":"1Yif4BpHosm-MYTckDeijbmWlWtY4THPL","timestamp":1638597755740},{"file_id":"1E6NJe6Uk5KclG8jj7gOonhcTGmckyo0o","timestamp":1638318205218},{"file_id":"1rgeX2vwLJRMMGe25sGHUk9lYR2D5dLoG","timestamp":1637878749591}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"}},"nbformat":4,"nbformat_minor":0}