{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "Project2_Part1_GroupF_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H08Y8cF2rr10",
        "FyGn77OJrx2Q"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ksioPziChR"
      },
      "source": [
        "Took the first 3 blocks of code from Yerlan's LC Compression Collab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlMXQhMKh2-B"
      },
      "source": [
        "#! git clone https://github.com/UCMerced-ML/LC-model-compression"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZELCKkPh5eT"
      },
      "source": [
        "#! pip3 install -e ./LC-model-compression"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbioGuw1h8oU"
      },
      "source": [
        "## IMPORTANT!\n",
        "At this point you need to restart the runtime by doing \"Runtime => Restart Runtime\"\n",
        "\n",
        "After doing the restart, I recommend just commenting out the 2 lines of code above so that we can press run all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3iOsCuGrNmv"
      },
      "source": [
        "## Training the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO9hxJGVrSE0"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qKzjYwwIfHF"
      },
      "source": [
        "#%matplotlib inline\n",
        "\n",
        "import lc\n",
        "from lc.torch import ParameterTorch as Param, AsVector, AsIs\n",
        "from lc.compression_types import ConstraintL0Pruning, LowRank, RankSelection, AdaptiveQuantization\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "from timeit import default_timer as timer\n",
        "from datetime import timedelta\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "torch.set_num_threads(4)\n",
        "batchsize = 2048\n",
        "num_workers = 2\n",
        "nnloss = torch.nn.CrossEntropyLoss()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDPGyBFMrVYn"
      },
      "source": [
        "### Making the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz-LJCYo1Mif"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw2vtabYQ_rA"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(28*28, 500)  # 28*28 from image dimension \n",
        "        self.fc2 = nn.Linear(500, 300)\n",
        "        self.fc3 = nn.Linear(300, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K_C6_DzrYn4"
      },
      "source": [
        "### Getting the subset MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_oj8JgSZs83"
      },
      "source": [
        "def data_loader(batch_size = batchsize, n_workers = num_workers):\n",
        "    train_data_th = datasets.MNIST(root='./datasets', download=True, train=True)\n",
        "    test_data_th = datasets.MNIST(root='./datasets', download=True, train=False)\n",
        "\n",
        "    label = [1, 2, 3 ,4, 6]\n",
        "    data_train_fea = np.array(train_data_th.data[:]).reshape([-1, 28 * 28]).astype(np.float32)\n",
        "    data_train_fea = (data_train_fea / 255)\n",
        "    data_train_gnd = np.array(train_data_th.targets)\n",
        "    ctr1_idx = np.where(data_train_gnd[:] == label[0])\n",
        "    ctr2_idx = np.where(data_train_gnd[:] == label[1])\n",
        "    ctr3_idx = np.where(data_train_gnd[:] == label[2])\n",
        "    ctr4_idx = np.where(data_train_gnd[:] == label[3])\n",
        "    ctr6_idx = np.where(data_train_gnd[:] == label[4])\n",
        "    ctr1_idx = np.array(ctr1_idx)\n",
        "    ctr2_idx = np.array(ctr2_idx)\n",
        "    ctr3_idx = np.array(ctr3_idx)\n",
        "    ctr4_idx = np.array(ctr4_idx)\n",
        "    ctr6_idx = np.array(ctr6_idx)\n",
        "    total_train_valid_idx = np.concatenate((ctr1_idx, ctr2_idx, ctr3_idx, ctr4_idx, ctr6_idx),axis = None)\n",
        "    np.random.shuffle(total_train_valid_idx)#forgot to add this line of code\n",
        "    train_idx = total_train_valid_idx[:27000]\n",
        "    valid_idx = total_train_valid_idx[27000:]\n",
        "\n",
        "    data_total_train = data_train_fea[total_train_valid_idx]\n",
        "    target_total_train = data_train_gnd[total_train_valid_idx]\n",
        "\n",
        "    data_train = data_train_fea[train_idx]\n",
        "    target_train = data_train_gnd[train_idx]\n",
        "\n",
        "    data_validation = data_train_fea[valid_idx]\n",
        "    target_validation = data_train_gnd[valid_idx]\n",
        "\n",
        "    data_test_fea = np.array(test_data_th.data[:]).reshape([-1, 28 * 28]).astype(np.float32)\n",
        "    data_test_fea = (data_test_fea / 255)\n",
        "    data_test_gnd = np.array(test_data_th.targets)\n",
        "    cte1_idx = np.where(data_test_gnd[:] == label[0])\n",
        "    cte2_idx = np.where(data_test_gnd[:] == label[1])\n",
        "    cte3_idx = np.where(data_test_gnd[:] == label[2])\n",
        "    cte4_idx = np.where(data_test_gnd[:] == label[3])\n",
        "    cte6_idx = np.where(data_test_gnd[:] == label[4])\n",
        "    cte1_idx = np.array(cte1_idx)\n",
        "    cte2_idx = np.array(cte2_idx)\n",
        "    cte3_idx = np.array(cte3_idx)\n",
        "    cte4_idx = np.array(cte4_idx)\n",
        "    cte6_idx = np.array(cte6_idx)\n",
        "    test_idx = np.concatenate((cte1_idx, cte2_idx, cte3_idx, cte4_idx, cte6_idx),axis = None)\n",
        "\n",
        "    data_test = data_test_fea[test_idx]\n",
        "    target_test = data_test_gnd[test_idx]\n",
        "\n",
        "    ##not sure what this is doing but it was here in both the neural network tutorial and in yerlan's collab\n",
        "    dtrain_mean = data_train.mean(axis=0)\n",
        "    data_train -= dtrain_mean\n",
        "    data_validation -=dtrain_mean\n",
        "    data_total_train -= dtrain_mean\n",
        "    data_test -= dtrain_mean\n",
        "    ##\n",
        "\n",
        "    #######\n",
        "    #https://discuss.pytorch.org/t/indexerror-target-2-is-out-of-bounds/69614/24\n",
        "    tensor_target_train = torch.from_numpy(target_train)\n",
        "\n",
        "    #print(tensor_target_train.size())\n",
        "    # print(min(tensor_target_train))\n",
        "    # print(max(tensor_target_train))\n",
        "    unique_targets_train = torch.unique(tensor_target_train) #1,2,3,4,6\n",
        "    # print('unique_targets_train: {}'.format(unique_targets_train))\n",
        "\n",
        "    new_tensor_target_train = torch.empty_like(tensor_target_train) #size of tensor_target_train\n",
        "    for idx, t in enumerate(unique_targets_train):\n",
        "        # print('replacing {} with {}'.format(t, idx))\n",
        "        new_tensor_target_train[tensor_target_train == t] = idx # [1,1,3,3]\n",
        "    # print(new_tensor_target_train.size())\n",
        "    # print(min(new_tensor_target_train))\n",
        "    # print(max(new_tensor_target_train))\n",
        "\n",
        "    tensor_target_validation = torch.from_numpy(target_validation)\n",
        "    #print(tensor_target_validation.size())\n",
        "    unique_targets_validation = torch.unique(tensor_target_validation)\n",
        "    new_tensor_target_validation = torch.empty_like(tensor_target_validation)\n",
        "    for idx, t in enumerate(unique_targets_validation):\n",
        "      new_tensor_target_validation[tensor_target_validation == t] = idx\n",
        "\n",
        "    tensor_target_test = torch.from_numpy(target_test)\n",
        "    unique_targets_test = torch.unique(tensor_target_test)\n",
        "    new_tensor_target_test = torch.empty_like(tensor_target_test)\n",
        "    for idx, t in enumerate(unique_targets_test):\n",
        "      new_tensor_target_test[tensor_target_test == t] = idx\n",
        "    \n",
        "\n",
        "    tensor_target_total_train = torch.from_numpy(target_total_train)\n",
        "    unique_targets_total_train = torch.unique(tensor_target_total_train)\n",
        "    new_tensor_target_total_train = torch.empty_like(tensor_target_total_train)\n",
        "    for idx, t in enumerate(unique_targets_total_train):\n",
        "      new_tensor_target_total_train[tensor_target_total_train == t] = idx\n",
        "\n",
        "\n",
        "    train_data = TensorDataset(torch.from_numpy(data_train), new_tensor_target_train)\n",
        "    validation_data = TensorDataset(torch.from_numpy(data_validation), new_tensor_target_validation)\n",
        "    test_data = TensorDataset(torch.from_numpy(data_test), new_tensor_target_test)\n",
        "    total_train_data = TensorDataset(torch.from_numpy(data_total_train), new_tensor_target_total_train)\n",
        "\n",
        "    train_loader = DataLoader(train_data, num_workers=n_workers, batch_size=batch_size, shuffle=True)\n",
        "    validation_loader = DataLoader(validation_data, num_workers = n_workers, batch_size = batch_size, shuffle = True)\n",
        "    test_loader = DataLoader(test_data, num_workers=n_workers, batch_size=batch_size, shuffle=False)\n",
        "    total_train_loader = DataLoader(total_train_data, num_workers = n_workers, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    #return train_loader, validation_loader, test_loader, total_train_loader -> use this one if still training model\n",
        "    return total_train_loader, test_loader # use this one if already found optimized model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPhDvBFfrhb7"
      },
      "source": [
        "### Defining all the lists used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yywffSSQoLBl"
      },
      "source": [
        "plotepoch = []\n",
        "plottrainloss = []\n",
        "plotvalidloss = []\n",
        "avg_train_loss = []\n",
        "avg_valid_loss = []\n",
        "plottrainacc = []\n",
        "plotvalidacc = []\n",
        "timetaken = []\n",
        "loss_list = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ez4PNdY_CnD"
      },
      "source": [
        "def clearList():\n",
        "    plotepoch.clear()\n",
        "    plottrainloss.clear()\n",
        "    plotvalidloss.clear()\n",
        "    avg_train_loss.clear()\n",
        "    avg_valid_loss.clear()\n",
        "    plottrainacc.clear()\n",
        "    plotvalidacc.clear()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUN-nkcBrfB1"
      },
      "source": [
        "### Calculating accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6mJjdV6aLt4"
      },
      "source": [
        "def calc_acc(loader, net):\n",
        "    correct_cnt = 0\n",
        "    total_cnt = 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_labels in loader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(dtype=torch.long, device=device)\n",
        "            out = net(batch_inputs)\n",
        "            _, pred_labels = torch.max(out.data, 1)\n",
        "            total_cnt += batch_labels.size(0)\n",
        "            correct_cnt += (pred_labels == batch_labels).sum().item()\n",
        "    \n",
        "\n",
        "    accuracy = correct_cnt / total_cnt\n",
        "    return accuracy\n",
        "\n",
        "def calc_acc_loss(loader, net):\n",
        "    correct_cnt = 0\n",
        "    total_cnt = 0\n",
        "    loss_list.clear()\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_labels in loader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_labels = batch_labels.to(dtype=torch.long, device=device)\n",
        "            out = net(batch_inputs)\n",
        "            loss = nnloss(out,batch_labels)\n",
        "            loss_list.append(loss.item())\n",
        "            _, pred_labels = torch.max(out.data, 1)\n",
        "            total_cnt += batch_labels.size(0)\n",
        "            correct_cnt += (pred_labels == batch_labels).sum().item()\n",
        "    \n",
        "    calc_loss = np.mean(loss_list)\n",
        "    accuracy = correct_cnt / total_cnt\n",
        "    return accuracy, calc_loss      "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nea1fzQ57g0e"
      },
      "source": [
        "def findAccNoTest(train_loader, validation_loader, net):\n",
        "    print(f'Train Accuracy: {100 * calc_acc(train_loader,net):.2f}%')\n",
        "    print(f'Validation Accuracy: {100 * calc_acc(validation_loader,net):.2f}%')\n",
        "\n",
        "def findAccWithTest(train_loader, validation_loader, test_loader, net):\n",
        "    print(f'Train Accuracy: {100 * calc_acc(train_loader,net):.2f}%')\n",
        "    print(f'Validation Accuracy: {100 * calc_acc(validation_loader,net):.2f}%')\n",
        "    print(f'Test Accuracy: {100 *calc_acc(test_loader,net):.2f}%')\n",
        "\n",
        "def findAccTrainTest(total_train_loader,  test_loader, net):\n",
        "    print(f'Train Accuracy: {100 * calc_acc(total_train_loader,net):.2f}%')\n",
        "    print(f'Test Accuracy: {100 *calc_acc(test_loader,net):.2f}%')\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-pyKKF6vQR8"
      },
      "source": [
        "def train_test_acc_eval_f(net):\n",
        "    train_loader, test_loader = data_loader()\n",
        "    acc_train, loss_train = calc_acc_loss(train_loader,net)\n",
        "    acc_test, loss_test = calc_acc_loss(test_loader,net)\n",
        "\n",
        "    print(f\"Train err: {100-acc_train*100:.2f}%, train loss: {loss_train}\")\n",
        "    print(f\"TEST ERR: {100-acc_test*100:.2f}%, test loss: {loss_test}\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08Y8cF2rr10"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hEht7fEFOds"
      },
      "source": [
        "#https://christianbernecker.medium.com/how-to-create-a-confusion-matrix-in-pytorch-38d06a7f04b7\n",
        "\n",
        "def confusion_matrix1(test_loader):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    for x, target in test_loader:\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.cuda()[:]\n",
        "            target = target.cuda().to(dtype=torch.long)\n",
        "        out = net(x)\n",
        "        out = (torch.max(torch.exp(out),1)[1]).data.cpu().numpy()\n",
        "        y_pred.extend(out)\n",
        "        target = target.data.cpu().numpy()\n",
        "        y_true.extend(target)\n",
        "\n",
        "    labels = [1,2,3,4,6]\n",
        "    cf_matrix = confusion_matrix(y_true,y_pred)\n",
        "    df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) * 10, index = [i for i in labels], columns = [i for i in labels])\n",
        "    plt.figure(figsize = (12,7))\n",
        "    sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyGn77OJrx2Q"
      },
      "source": [
        "### Yerlan's Suggestions for Project 2 Part 1 Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPOBUUwuZc55"
      },
      "source": [
        "#select reasonable settings\n",
        "#every 10 epoch 0.9 schedular\n",
        "#have 60k training\n",
        "#10k test\n",
        "#choose lr: 0.01 - 0.1\n",
        "# lr decay: 0.95 every 10 epoch\n",
        "# epoch 100-150\n",
        "#sgd: use momentum\n",
        "# for momentum: default 0.9 okay\n",
        "# 5 - 10 experiments\n",
        "#once found best parameters\n",
        "#use them on entire dataset 60k\n",
        "#split training 55k,5k train/valid\n",
        "#in our case split training 27k,rest train/valid\n",
        "#once found best parameters\n",
        "#train model on all training\n",
        "#report true test error on 10k only once\n",
        "#use large as possible batch size that fits in gpu allocates like 2000 or 2048"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpH_7dLlr00n"
      },
      "source": [
        "### Training the Netural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sU0jY3FYt9d"
      },
      "source": [
        "# uncomment to test parameters again\n",
        "\n",
        "# nnloss = torch.nn.CrossEntropyLoss()\n",
        "# batchsize = 2048\n",
        "# #if i set num_workers = 4 i get this warning\n",
        "# #/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked))\n",
        "# #So I will do what they suggest and switch to 2\n",
        "# num_workers = 2\n",
        "# timein = \"tmp\"\n",
        "# train_loader, validation_loader, test_loader, total_train_loader = data_loader(batchsize,num_workers)\n",
        "\n",
        "def train_net(net):\n",
        "    params = list(filter(lambda p: p.requires_grad, net.parameters()))\n",
        "    optimizer = optim.SGD(params, lr=0.1, momentum=0.9, weight_decay = 0, nesterov=True)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "    \n",
        "    epochs = 100\n",
        "    clearList()\n",
        "    start = timer()\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        for x, target in train_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.cuda()[:]\n",
        "                target = target.cuda().to(dtype=torch.long)\n",
        "            optimizer.zero_grad()\n",
        "            out = net(x)\n",
        "            loss = nnloss(out, target)\n",
        "            loss.backward()\n",
        "            avg_train_loss.append(loss.item())\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "        train_loss = np.mean(avg_train_loss)\n",
        "        plotepoch.append(epoch)\n",
        "        plottrainloss.append(train_loss)\n",
        "        \n",
        "        #https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, target in validation_loader:\n",
        "                if torch.cuda.is_available():\n",
        "                    x = x.cuda()[:]\n",
        "                    target = target.cuda().to(dtype=torch.long)\n",
        "                out = net(x)\n",
        "                loss = nnloss(out, target)\n",
        "                avg_valid_loss.append(loss.item())\n",
        "        valid_loss = np.mean(avg_valid_loss)\n",
        "        plotvalidloss.append(valid_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"\\tepoch #{epoch} is finished.\")\n",
        "            print(f\"\\t  Avg. Train loss: {train_loss}\")\n",
        "            print(f\"\\t  Avg. Validation loss: {valid_loss}\")\n",
        "        \n",
        "        train_acc = calc_acc(train_loader, net)\n",
        "        plottrainacc.append(train_acc)\n",
        "        valid_acc = calc_acc(validation_loader, net)\n",
        "        plotvalidacc.append(valid_acc)        \n",
        "\n",
        "    fig = plt.figure(1)\n",
        "    plt.plot(plotepoch,plottrainloss,color = \"blue\", label = \"Average Train Loss\")\n",
        "    plt.plot(plotepoch,plotvalidloss,color = \"red\", label = \"Average Validation Loss\")\n",
        "    plt.title('Epoch vs Train and Validation Average Loss')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.xlabel(\"Increasing Epoch Value by 1\")\n",
        "    plt.ylabel(\"Avg Loss\")\n",
        "    plt.show()\n",
        "    fig2 = plt.figure(2)\n",
        "    plt.plot(plottrainacc, color=\"blue\", label=\"Train Accuracy\")\n",
        "    plt.plot(plotvalidacc, color=\"red\", label=\"Validation Accuracy\")\n",
        "    plt.title('Epoch vs Train and Validation Accuracy')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.xlabel(\"Increasing Epoch Value by 1\")\n",
        "    plt.ylabel(\"Accuracy Score\")\n",
        "    plt.show()\n",
        "    print(\"After optimizing the model\")\n",
        "    findAccWithTest(train_loader, validation_loader, test_loader, net)\n",
        "    \n",
        "    end = timer()\n",
        "\n",
        "    #https://stackoverflow.com/questions/7370801/how-to-measure-elapsed-time-in-python\n",
        "    taken=(end-start)\n",
        "    timetaken.append(taken)\n",
        "    if(taken > 60):\n",
        "        timein = \"minutes\"\n",
        "    else:\n",
        "        timein = \"seconds\"\n",
        "    print(f\"It took us {timedelta(seconds=taken)} {timein} to run this loop\")\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw4o3gdRiGio"
      },
      "source": [
        "#uncomment this code if we need to train the model\n",
        "# print(\"Before optimizing the model\")\n",
        "# findAccNoTest(train_loader, validation_loader, net)\n",
        "# print(\"Training the Model\")\n",
        "# train_net(net)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQBBeGUsr35W"
      },
      "source": [
        "### Code to Run the Optimized Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JYfpIoEmg6i"
      },
      "source": [
        "# The code above was for testing and find the best parameters\n",
        "\n",
        "# For training the model you only need the code down below\n",
        "def best_train_net(net):\n",
        "    params = list(filter(lambda p: p.requires_grad, net.parameters()))\n",
        "    optimizer = optim.SGD(params, lr=0.1, momentum=0.9, weight_decay = 0, nesterov=True)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
        "    train_loader, test_loader = data_loader()\n",
        "    print(\"Before optimizing the model\")\n",
        "    print(f'Train Accuracy: {100 * calc_acc(train_loader,net):.2f}%')\n",
        "        \n",
        "    epochs = 100\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        for x, target in train_loader:\n",
        "            x = x.to(device)\n",
        "            target = target.to(dtype=torch.long, device=device)\n",
        "            optimizer.zero_grad()\n",
        "            out = net(x)\n",
        "            loss = nnloss(out, target)\n",
        "            loss.backward()\n",
        "            avg_train_loss.append(loss.item())\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "        train_loss = np.mean(avg_train_loss)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"\\tepoch #{epoch} is finished.\")\n",
        "            print(f\"\\t  Avg. Train loss: {train_loss}\")\n",
        "\n",
        "    print(\"After optimizing the model\")\n",
        "    findAccTrainTest(train_loader, test_loader, net)\n",
        "    confusion_matrix1(test_loader)\n",
        "    torch.save(net.state_dict(), \"best_parameter_model.pth\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEIEP-jjkMww",
        "outputId": "0fe323af-805d-4188-dc93-247226f6abbc"
      },
      "source": [
        "best_train_net(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before optimizing the model\n",
            "Train Accuracy: 18.64%\n",
            "\tepoch #0 is finished.\n",
            "\t  Avg. Train loss: 1.3208674669265748\n",
            "\tepoch #10 is finished.\n",
            "\t  Avg. Train loss: 0.19842685000462965\n",
            "\tepoch #20 is finished.\n",
            "\t  Avg. Train loss: 0.11505755827658706\n",
            "\tepoch #30 is finished.\n",
            "\t  Avg. Train loss: 0.08148298279973128\n",
            "\tepoch #40 is finished.\n",
            "\t  Avg. Train loss: 0.06311564424219049\n",
            "\tepoch #50 is finished.\n",
            "\t  Avg. Train loss: 0.05152207128855771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoWTjGR5x9NK"
      },
      "source": [
        "train_test_acc_eval_f(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qem-bVZeFI2u"
      },
      "source": [
        "def load_reference_net():\n",
        "    state_dict = torch.load(\"best_parameter_model.pth\")\n",
        "    net.load_state_dict(state_dict)\n",
        "    net.eval()\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLos7uzEhq8k"
      },
      "source": [
        "Below is for Project 2 Part 1 Part 2 LC Compression\n",
        "\n",
        "All the code was taken from Yerlan's LC Compression Collab and modified to fit the parameters of the project and our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym1nubjvqoM7"
      },
      "source": [
        "## Compression using the LC toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_xswZO9iQ50"
      },
      "source": [
        "### Step 1: L step\n",
        "We will use same L step with same hyperparamters for all our compression examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-vYCfwPiTqX"
      },
      "source": [
        "def my_l_step(model, lc_penalty, step):\n",
        "    train_loader, test_loader = data_loader()\n",
        "    params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    lr = 0.1*(0.98**step)\n",
        "    optimizer = optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\n",
        "    print(f'L-step #{step} with lr: {lr:.5f}')\n",
        "    epochs_per_step_ = 7\n",
        "    if step == 0:\n",
        "        epochs_per_step_ = epochs_per_step_ * 2\n",
        "    for epoch in range(epochs_per_step_):\n",
        "        avg_loss = []\n",
        "        for x, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            x = x.to(device)\n",
        "            target = target.to(dtype=torch.long, device=device)\n",
        "            out = model(x)\n",
        "            #loss = model.loss(out, target) + lc_penalty()\n",
        "            loss = nnloss(out,target) + lc_penalty()\n",
        "            avg_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"\\tepoch #{epoch} is finished.\")\n",
        "        print(f\"\\t  avg. train loss: {np.mean(avg_loss):.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lzUNbRdiXKA"
      },
      "source": [
        "### Step 2: Schedule of mu values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKr30OY6iZCr"
      },
      "source": [
        "mu_s = [9e-5 * (1.1 ** n) for n in range(20)]\n",
        "# 20 L-C steps in total\n",
        "# total training epochs is 7 x 20 = 140"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z831i_Dkic5I"
      },
      "source": [
        "### Compression time! Pruning\n",
        "Let us prune all but 5% of the weights in the network (5% = 27175 weights)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BIRvtimidfn"
      },
      "source": [
        "net = load_reference_net()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "compression_tasks = {\n",
        "    Param(layers, device): (AsVector, ConstraintL0Pruning(kappa=27175), 'pruning')\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()                              # entry point to the LC algorithm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2HG4UQ7iihx"
      },
      "source": [
        "lc_alg.count_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sTPIJlXikJg"
      },
      "source": [
        "compressed_model_bits = lc_alg.count_param_bits() + (300+100+10)*32\n",
        "uncompressed_model_bits = (784*300+300*100+100*10 + 300 + 100 + 10)*32\n",
        "compression_ratio = uncompressed_model_bits/compressed_model_bits\n",
        "print(compression_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICRfZbzfimWK"
      },
      "source": [
        "Note that we were pruning 95% of the weights. Naively, you would assume 20x compression ratio (100%/5%), however, this is not the case. Firstly, there are some uncompressed parts (in this case biases), and, secondly, storing a compressed model requires additional metadata (in this case positions of non-zero elements). Therefore we get only 16x compression ratio (vs naively expected 20x). \n",
        "\n",
        "To prevent manual computation of compression ratio, let us create a function below. Note, this function is model specific."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i6YyE4WiokY"
      },
      "source": [
        "def compute_compression_ratio(lc_alg):\n",
        "    compressed_model_bits = lc_alg.count_param_bits() + (300+100+10)*32\n",
        "    uncompressed_model_bits = (784*300+300*100+100*10 + 300 + 100 + 10)*32\n",
        "    compression_ratio = uncompressed_model_bits/compressed_model_bits\n",
        "    return compression_ratio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwVUkcZlisac"
      },
      "source": [
        "### Quantization\n",
        "Now let us quantize each layer with its own codebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXfFy3ZqivYb"
      },
      "source": [
        "net = load_reference_net()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "\n",
        "compression_tasks = {\n",
        "    Param(layers[0], device): (AsVector, AdaptiveQuantization(k=2), 'layer0_quant'),\n",
        "    Param(layers[1], device): (AsVector, AdaptiveQuantization(k=2), 'layer1_quant'),\n",
        "    Param(layers[2], device): (AsVector, AdaptiveQuantization(k=2), 'layer2_quant')\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()  \n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB9N1SnniyVB"
      },
      "source": [
        "### Mixing pruning, low rank, and quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2JOit3Pi0X_"
      },
      "source": [
        "net = load_reference_net()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "\n",
        "compression_tasks = {\n",
        "    Param(layers[0], device): (AsVector, ConstraintL0Pruning(kappa=5000), 'pruning'),\n",
        "    Param(layers[1], device): (AsIs, LowRank(target_rank=9, conv_scheme=None), 'low-rank'),\n",
        "    Param(layers[2], device): (AsVector, AdaptiveQuantization(k=2), 'quant')\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()\n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g82WI8vti2o6"
      },
      "source": [
        "### Additive combination of Quantization and Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NbF1OFNi4il"
      },
      "source": [
        "net = load_reference_net()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "\n",
        "compression_tasks = {\n",
        "    Param(layers, device): [\n",
        "        (AsVector, ConstraintL0Pruning(kappa=2662), 'pruning'),\n",
        "        (AsVector, AdaptiveQuantization(k=2), 'quant')\n",
        "    ]\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()\n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOf5GiCEi68m"
      },
      "source": [
        "### Low-rank compression with automatic rank selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojI-n6a5i96p"
      },
      "source": [
        "net = load_reference_net()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "alpha=1e-9\n",
        "compression_tasks = {\n",
        "    Param(layers[0], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[0], normalize=True), \"layer1_lr\"),\n",
        "    Param(layers[1], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[1], normalize=True), \"layer2_lr\"),\n",
        "    Param(layers[2], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[2], normalize=True), \"layer3_lr\")\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()\n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiaWzjDNjAWW"
      },
      "source": [
        "### ScaledBinaryQuantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZiLDe35jBmh"
      },
      "source": [
        "from lc.compression_types import ScaledBinaryQuantization\n",
        "net = load_reference_net()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "\n",
        "compression_tasks = {\n",
        "    Param(layers[0], device): (AsVector, ScaledBinaryQuantization(), 'layer0_quant'),\n",
        "    Param(layers[1], device): (AsVector, ScaledBinaryQuantization(), 'layer1_quant'),\n",
        "    Param(layers[2], device): (AsVector, ScaledBinaryQuantization(), 'layer2_quant')\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run()\n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ3ivw1sjDaT"
      },
      "source": [
        "### ScaledTernaryQuantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWWysxWcjEqY"
      },
      "source": [
        "from lc.compression_types import ScaledTernaryQuantization\n",
        "net = load_reference_net()\n",
        "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear)]\n",
        "\n",
        "compression_tasks = {\n",
        "    Param(layers[0], device): (AsVector, ScaledTernaryQuantization(), 'layer0_quant'),\n",
        "    Param(layers[1], device): (AsVector, ScaledTernaryQuantization(), 'layer1_quant'),\n",
        "    Param(layers[2], device): (AsVector, ScaledTernaryQuantization(), 'layer2_quant')\n",
        "}\n",
        "\n",
        "lc_alg = lc.Algorithm(\n",
        "    model=net,                            # model to compress\n",
        "    compression_tasks=compression_tasks,  # specifications of compression\n",
        "    l_step_optimization=my_l_step,        # implementation of L-step\n",
        "    mu_schedule=mu_s,                     # schedule of mu values\n",
        "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
        ")\n",
        "lc_alg.run() \n",
        "print('Compressed_params:', lc_alg.count_params())\n",
        "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}